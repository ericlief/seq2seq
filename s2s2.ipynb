{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install --upgrade torchtext\n",
    "# !pip install --upgrade torch\n",
    "\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-82-b3cccb854e4a>:5: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
      "  pd.set_option(\"display.max_colwidth\", -1)\n"
     ]
    }
   ],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount(\"/content/gdrive\")\n",
    "\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", -1)\n",
    "# path = \"/content/gdrive/My Drive/data/amazon/amazon-reviews-max50-37k.csv\"\n",
    "path = \"data/amazon-reviews-max50_half.csv\"\n",
    "df = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Since putting our dog on the Wellness dog food, we don't have any more diarrhea or upset tummy issues.</td>\n",
       "      <td>So happy with Wellness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I was not a huge granola fan until a friend turned me onto Dr FLynn's creation. It's not only healthier than the rest, but it tastes so much better! The best of both worlds.  Can't get enough especially since it's organic! Thank you Dr Flynn!!!</td>\n",
       "      <td>Best Granola on the Planet!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>OMG I cannot even begin to descibe how delicious these bars are! It is like an almond joy without the chocolate! YUM!!!</td>\n",
       "      <td>YUM!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The flavor of the salt is very nice ! Purchased it upon reading about it's many health benefits. The price is very reasonable as well. In local health food store it costs twice as much.</td>\n",
       "      <td>Very Happy With This Product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Originally found these in a candy store in Tarrytown,NY,&lt;br /&gt;then was delighted to find them  on Amazon.&lt;br /&gt;They are gluten free and soothe my often-irritated throat.&lt;br /&gt;I love them and love purchasing them at Amazon with the Special Savings Prog.</td>\n",
       "      <td>Honey Candies</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                         source  \\\n",
       "0  Since putting our dog on the Wellness dog food, we don't have any more diarrhea or upset tummy issues.                                                                                                                                                         \n",
       "1  I was not a huge granola fan until a friend turned me onto Dr FLynn's creation. It's not only healthier than the rest, but it tastes so much better! The best of both worlds.  Can't get enough especially since it's organic! Thank you Dr Flynn!!!           \n",
       "2  OMG I cannot even begin to descibe how delicious these bars are! It is like an almond joy without the chocolate! YUM!!!                                                                                                                                        \n",
       "3  The flavor of the salt is very nice ! Purchased it upon reading about it's many health benefits. The price is very reasonable as well. In local health food store it costs twice as much.                                                                      \n",
       "4  Originally found these in a candy store in Tarrytown,NY,<br />then was delighted to find them  on Amazon.<br />They are gluten free and soothe my often-irritated throat.<br />I love them and love purchasing them at Amazon with the Special Savings Prog.   \n",
       "\n",
       "                          target  \n",
       "0  So happy with Wellness         \n",
       "1  Best Granola on the Planet!!!  \n",
       "2  YUM!                           \n",
       "3  Very Happy With This Product   \n",
       "4  Honey Candies                  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>123631</td>\n",
       "      <td>123631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>94250</td>\n",
       "      <td>69813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Best one if you are looking for pure St John Wort Tea, all the other St John tea have other items in them hence you effects like high blood pressure get this instead it is better then a regular anti depressant and works right away</td>\n",
       "      <td>Delicious</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>18</td>\n",
       "      <td>724</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                        source  \\\n",
       "count   123631                                                                                                                                                                                                                                   \n",
       "unique  94250                                                                                                                                                                                                                                    \n",
       "top     Best one if you are looking for pure St John Wort Tea, all the other St John tea have other items in them hence you effects like high blood pressure get this instead it is better then a regular anti depressant and works right away   \n",
       "freq    18                                                                                                                                                                                                                                       \n",
       "\n",
       "           target  \n",
       "count   123631     \n",
       "unique  69813      \n",
       "top     Delicious  \n",
       "freq    724        "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "123631"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liefe/.virtualenvs/s2s/lib/python3.8/site-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
      "/home/liefe/.virtualenvs/s2s/lib/python3.8/site-packages/torchtext/data/example.py:68: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n",
      "/home/liefe/.virtualenvs/s2s/lib/python3.8/site-packages/torchtext/data/example.py:78: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n"
     ]
    }
   ],
   "source": [
    "from torchtext.data import Field, TabularDataset\n",
    "\n",
    "TEXT = Field(use_vocab=True, init_token=\"<SOS>\", eos_token=\"<EOS>\",\n",
    "             fix_length=25, tokenize=\"spacy\",\n",
    "             include_lengths=True, batch_first=True, lower=True,\n",
    "             is_target=False\n",
    "             )\n",
    "\n",
    "fields = [(\"source\", TEXT),\n",
    "          (\"target\", TEXT)]\n",
    "\n",
    "data = TabularDataset(path, \"CSV\", fields, skip_header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_data, test_data = data.split(.70)\n",
    "train_data, val_data = train_data.split(.90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(77888, 8654, 37089)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data), len(val_data), len(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# TEXT.build_vocab(train_data, vectors=\"fasttext.en.300d\")\n",
    "# TEXT.build_vocab(train_data, vectors=\"fasttext.en.300d\")\n",
    "# TEXT.build_vocab(train_data, vectors=\"glove.6B.100d\", max_size=10000, min_freq=3)\n",
    "TEXT.build_vocab(train_data, max_size=10000, min_freq=3)\n",
    "\n",
    "#  Next time max_size=10000, min_freq=3 ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens in source/target vocabulary: 10004\n"
     ]
    }
   ],
   "source": [
    "print(f\"Unique tokens in source/target vocabulary: {len(TEXT.vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['source', 'target'])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[0].__dict__.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['love',\n",
       "  'the',\n",
       "  'ease',\n",
       "  'of',\n",
       "  'having',\n",
       "  'green',\n",
       "  'natural',\n",
       "  'tea',\n",
       "  'anywhear',\n",
       "  'any',\n",
       "  'time',\n",
       "  '.',\n",
       "  'tastes',\n",
       "  'great',\n",
       "  'i',\n",
       "  'love',\n",
       "  'this',\n",
       "  'stash',\n",
       "  'green',\n",
       "  'tea'],\n",
       " ['great', 'product'])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0].source, train_data[0].target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(98130, 67549, 4)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.vocab.freqs[\"the\"], TEXT.vocab.freqs[\"a\"], TEXT.vocab.freqs[\"shit\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 'the')"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.vocab.stoi[\"the\"], TEXT.vocab.itos[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([['<SOS>', 'Eric', 'needs', 'to', 'leave', '<EOS>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']], [6])\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "batch = [\"Eric needs to leave\".split()]\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "pad = TEXT.pad(batch)\n",
    "print(pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70735\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([['<SOS>',\n",
       "   'great',\n",
       "   'lollipops',\n",
       "   '...',\n",
       "   'taste',\n",
       "   'exactlyl',\n",
       "   'like',\n",
       "   'watermelon',\n",
       "   '!',\n",
       "   ' ',\n",
       "   'used',\n",
       "   'them',\n",
       "   'for',\n",
       "   'centerpieces',\n",
       "   'for',\n",
       "   'my',\n",
       "   'sons',\n",
       "   'barmitzvah',\n",
       "   'and',\n",
       "   'the',\n",
       "   'kids',\n",
       "   'loved',\n",
       "   'them',\n",
       "   '!',\n",
       "   '<EOS>']],\n",
       " [25])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "i = random.randrange(0, len(train_data))\n",
    "print(i)\n",
    "source = train_data[i].source\n",
    "TEXT.pad([source])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['love',\n",
       " 'the',\n",
       " 'ease',\n",
       " 'of',\n",
       " 'having',\n",
       " 'green',\n",
       " 'natural',\n",
       " 'tea',\n",
       " 'anywhear',\n",
       " 'any',\n",
       " 'time',\n",
       " '.',\n",
       " 'tastes',\n",
       " 'great',\n",
       " 'i',\n",
       " 'love',\n",
       " 'this',\n",
       " 'stash',\n",
       " 'green',\n",
       " 'tea']"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0].source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['great', 'product']"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0].target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60524\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[   2,   13,  246,  583, 1011,   95,   20,   18,  188,  105, 1607,   24,\n",
       "            10,   58,  506,   11,    6,  452,   31,   14,  136,   10, 4014,  506,\n",
       "             3]]),\n",
       " tensor([25]))"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "i = random.randrange(0, len(train_data))\n",
    "print(i)\n",
    "source = train_data[i].source\n",
    "TEXT.process([source])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define\n",
    "iterator / batcher / loader\n",
    "here\n",
    "note\n",
    "that\n",
    "this is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% \n"
    }
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.source_dim = config[\"source_dim\"]  # vocab size/number of classes\n",
    "        self.embedding_dim = config[\"embedding_dim\"]\n",
    "        self.hidden_dim = config[\"hidden_dim\"]\n",
    "        self.layers = config.get(\"layers\", 1)\n",
    "        self.bidirectional_encoder = config.get(\"bidirectional_encoder\", False)\n",
    "\n",
    "        # Layers\n",
    "        self.embedding = nn.Embedding(self.source_dim, self.embedding_dim)\n",
    "        self.dropout = nn.Dropout(config.get(\"dropout\", 0.0))\n",
    "        self.gru = nn.GRU(self.embedding_dim, self.hidden_dim, dropout=config.get(\"dropout\", 0.0),\n",
    "                          num_layers=self.layers, bias=True, batch_first=True, bidirectional=self.bidirectional_encoder)\n",
    "        # self.gru = nn.GRU(64, 64, dropout=0, num_layers=1, bias=True, batch_first=True, bidirectional=False).to(device)\n",
    "\n",
    "    def forward(self, source, source_lens=None, pack_padded=False):\n",
    "\n",
    "        \"\"\"\n",
    "        :param source: [B, T]\n",
    "        :param source_lengths: [T]\n",
    "        :param pack_padded: bool\n",
    "        :return: output: [B, T, H], hidden: Union([1, B, H] or [2, B, H])\n",
    "        \"\"\"\n",
    "        # print(\"encoder forward\")\n",
    "        # print(input)\n",
    "\n",
    "        # Embed source index sequences [B, T] > [B, T, E]\n",
    "        embedded = self.embedding(source)\n",
    "\n",
    "        # print(embedded)\n",
    "\n",
    "        # Apply dropout\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        # Pack if padded sequences and lengths given\n",
    "\n",
    "        if pack_padded:\n",
    "\n",
    "            embedded = pack_padded_sequence(embedded, source_lens, batch_first=True, enforce_sorted=False)\n",
    "\n",
    "        # Map embedded source [B, T, E] to Union([B, T, H] or [B, T, 2*H], Union([1, B, H] or [2, B, H])\n",
    "        output, hidden = self.gru(embedded)  # TODO: add state tuple\n",
    "\n",
    "        if pack_padded:\n",
    "            output, output_lens = pad_packed_sequence(output, batch_first=True)\n",
    "\n",
    "        # Sum outputs of bidirectional RNN:\n",
    "        # [B, T, 2*H] > [B, T, H]\n",
    "        if self.bidirectional_encoder:\n",
    "            output = output[:, :, :self.hidden_dim] + output[:, :, self.hidden_dim:]\n",
    "\n",
    "        return output, hidden\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.source_dim = config[\"source_dim\"]  # vocab size/number of classes\n",
    "        self.embedding_dim = config[\"embedding_dim\"]  # Can be different from hidden!?\n",
    "        self.bidirectional_encoder = config.get(\"bidirectional_encoder\", False)\n",
    "        self.hidden_dim = config[\"hidden_dim\"]\n",
    "        # self.hidden_dim = 2 * config[\"hidden_dim\"] if self.bidirectional_encoder else config[\"hidden_dim\"]\n",
    "\n",
    "        # Layers\n",
    "        self.embedding = nn.Embedding(self.source_dim, self.embedding_dim)\n",
    "        self.dropout = nn.Dropout(config.get(\"dropout\", 0.0))\n",
    "        self.gru = nn.GRU(self.embedding_dim + self.hidden_dim, self.hidden_dim, dropout=config.get(\"dropout\", 0.0),\n",
    "                          bias=True, batch_first=True)\n",
    "        self.fc = nn.Linear(self.hidden_dim, self.source_dim)  # input classes = output classes\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "        # For attention\n",
    "        self.source_layer = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
    "        self.state_layer = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
    "        self.weight_layer = nn.Linear(self.hidden_dim, 1)\n",
    "\n",
    "    def forward(self, input, hidden, enc_outputs):\n",
    "        \"\"\"\n",
    "        Process batch inputs at one time step and decode prediction.\n",
    "\n",
    "        :param input: [B]\n",
    "        :param hidden: [1, B, H]\n",
    "        :param context_vec: [1, B, H]\n",
    "        :return: prediction (B, H), hidden (1, B, H)\n",
    "        \"\"\"\n",
    "\n",
    "        # print(\"decoder forward\")\n",
    "        # print(input)\n",
    "\n",
    "        # Process the batch inputs only at this timestep t\n",
    "        # Input of shape [B], but we want [B, 1]\n",
    "        input = input.unsqueeze(1)\n",
    "\n",
    "        # print(f\"input of shape {input.size()}\")\n",
    "        # print(f\"context vec of shape {context_vec.size()}\")\n",
    "\n",
    "        # Embed input [B, 1] > [B, 1, E]\n",
    "        # Note embedding dimension needs to equal hidden if concatenated\n",
    "        # But could map to same space\n",
    "        input = self.embedding(input)\n",
    "        # print(input)\n",
    "\n",
    "        # Apply attention and get next input\n",
    "        # [B, 1, 2*H]\n",
    "        next_input = self.attention(input, hidden, enc_outputs)\n",
    "\n",
    "        # Sum context vec and input ???padded = field.pad(minibatch)\n",
    "        # embedded = embedded + context_vec\n",
    "\n",
    "        # Apply dropout\n",
    "        # embedded = self.dropout(embedded)\n",
    "\n",
    "        # embedded = pack_padded_sequence(embedded, input_lens, batch_first=True, enforce_sorted=False)\n",
    "\n",
    "        # Feed to RNN\n",
    "        output, hidden = self.gru(next_input, hidden)\n",
    "\n",
    "        # print(f\"output==hidden {output[0][0]==hidden[0][0]}\")\n",
    "\n",
    "        # output, output_lens = pad_packed_sequence(output, batch_first=True)\n",
    "\n",
    "        # Shape of output: (B, 1, H), we want (B, H)\n",
    "        output = output.squeeze(1)\n",
    "\n",
    "        # Get distribution of classes\n",
    "        output = self.fc(output)\n",
    "\n",
    "        return output, hidden\n",
    "\n",
    "    def attention(self, input, hidden, enc_outputs):\n",
    "        # Calculate attention\n",
    "\n",
    "        # Calculate attention distribution alpha for all encoder outputs\n",
    "\n",
    "        # # Find attention using dot product score (source X hidden_target)\n",
    "        # Didn't work too well\n",
    "        # # Reshape target hidden: [1, B, H] > [B, H, 1]\n",
    "        # h_t = hidden.view(-1, self.hidden_dim, 1)\n",
    "        # # print(f\" s_t: {s_t.size()}\")\n",
    "\n",
    "        # # For batch, perform matrix mult of outputs hidden states and hidden state (s_t) at each time step\n",
    "        # # [B, T, H] X [B, H, 1] = [B, T, 1]\n",
    "        # # print(f\" outputs: {outputs.size()}\")\n",
    "        # score = enc_outputs.bmm(h_t)\n",
    "\n",
    "        # # Reshape [B, T, 1] > [B, 1, T] and take softmax\n",
    "        # alphas = self.softmax(score.transpose(1,2))\n",
    "        # # print(f\" alphas: {alphas.size()}\")\n",
    "\n",
    "        # # Find attention. Take weighted sum of hidden states (i.e. linear combo)\n",
    "        # # [B, 1, T] X [B, T, H] = [B, 1, H]\n",
    "        # context_vec = alphas.bmm(enc_outputs)\n",
    "\n",
    "        # Concatenate next input and context\n",
    "        # [B, 1, H] ; [B, 1, H] = [B, 1, 2*H]\n",
    "        # print(f\" context: {context_vec.size()}\")\n",
    "        # print(f\" in: {input.size()}\")\n",
    "\n",
    "        # Find attention using Bahdenah\n",
    "        # tanh(Vh_s + Wh_t + b)\n",
    "\n",
    "        # Project source layer: Vh_s\n",
    "        # [B, T, H] > [B, T, H]\n",
    "        source_layer = self.source_layer(enc_outputs)\n",
    "\n",
    "        # Reshape target hidden: [1, B, H] > [B, 1, H]\n",
    "        h_t = hidden.view(-1, 1, self.hidden_dim)\n",
    "\n",
    "        # Project target hidden state to layer\n",
    "        # [B, 1, H] > [B, 1, H]\n",
    "        state_layer = self.state_layer(h_t)\n",
    "\n",
    "        # tanh(Vh_s + Wh_t + b)\n",
    "        # [B, T, H] > [B, T, H]\n",
    "        score = torch.tanh(source_layer + state_layer)\n",
    "\n",
    "        # Project to weight layer with 1 unit\n",
    "        # [B, T, H] > [B, T, 1]\n",
    "        weights = self.weight_layer(score)\n",
    "\n",
    "        # print(weights[0])\n",
    "\n",
    "        weights = self.softmax(weights)\n",
    "\n",
    "        # print(weights[0])\n",
    "        # print(weights.size())\n",
    "        # print(torch.sum(weights, dim=1)[0])\n",
    "\n",
    "        # Find attention. Take weighted sum of hidden states (i.e. linear combo)\n",
    "        # [B, 1, T] X [B, T, H] = [B, 1, H]\n",
    "        context_vec = weights.view(weights.size(0), 1, -1).bmm(enc_outputs)\n",
    "\n",
    "        # print(context_vec.size())\n",
    "        next_input = torch.cat([input, context_vec], dim=-1)\n",
    "\n",
    "        return next_input\n",
    "        # Sum hidden and context_vec\n",
    "        # Didn't seem to work to well\n",
    "        # hidden = hidden + context_vec.view(1, bs, -1)\n",
    "\n",
    "        # Reshape [B, 1, H] > [1, B, H]\n",
    "        # hidden = context_vec.view(1, bs, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "\n",
    "        self.bidirectional_encoder = config.get(\"bidirectional_encoder\", False)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "        # Block\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, source, target=None, source_lens=None, pack_padded=False, teacher_force_ratio=.5, predict=False):\n",
    "        \"\"\"\n",
    "        :param source: [B, T]\n",
    "        :param target: [B, T]\n",
    "        :param source_lengths: [T]\n",
    "        :param pack_padded: bool\n",
    "        :return: output (B, V), context (B, H), prev_hidden (B, H), weights (B, T)\n",
    "        \"\"\"\n",
    "\n",
    "        target_vocab_size = self.encoder.source_dim  # here same, but may change for MT\n",
    "        hidden_size = self.decoder.hidden_dim  # Pain in ass 2 * encoder hidden size\n",
    "\n",
    "        # Encode, only need output for attention\n",
    "        # out: [B, T, H], for bidirectional torch automatically sums outputs\n",
    "        enc_outputs, hidden = self.encoder(source, source_lens, pack_padded)\n",
    "\n",
    "        # Concatenate hidden states for bidirection encoder\n",
    "        # [2, B, H] > [1, B, 2*H]\n",
    "        # if self.bidirectional_encoder:\n",
    "        #   hidden = torch.cat((hidden[0], hidden[1]), dim=-1)\n",
    "        #   # Add first dim\n",
    "        #   hidden = hidden.unsqueeze(0)\n",
    "\n",
    "        if self.bidirectional_encoder:\n",
    "            hidden = hidden[0] + hidden[1]\n",
    "            # Add first dim\n",
    "            hidden = hidden.unsqueeze(0)\n",
    "\n",
    "        # Iterate through outputs and decode\n",
    "        bs = source.size(0)\n",
    "\n",
    "        # Train\n",
    "        if not predict:\n",
    "\n",
    "\n",
    "            # Get start token for batch, e.g. [SOS ...]\n",
    "            target_length = target.size(1)  # includes padding (i.e max length) -- to be ignored by loss func\n",
    "\n",
    "            # [B, 1]\n",
    "            # print(f\" target: {target.size()}\")\n",
    "            input = target[:, 0]  # SOS\n",
    "            # print(f\" input: {input.size()}\")\n",
    "\n",
    "            # Get all output predictions step by step and store in matrix of shape [B, T, H]\n",
    "            predictions = torch.zeros(bs, target_length, target_vocab_size).to(device)\n",
    "            for t in range(target_length):\n",
    "                # Decode\n",
    "                output, hidden = self.decoder(input, hidden, enc_outputs)\n",
    "\n",
    "                # Get predicted output sequences for timestep\n",
    "                prediction = self.softmax(output)\n",
    "\n",
    "                # print(\"prediction\", prediction)\n",
    "\n",
    "                # [B, H] as softmax distribution\n",
    "                predictions[:, t] = prediction\n",
    "                prediction = prediction.argmax(1)\n",
    "\n",
    "                # print(f\"Predicted sequence {prediction}; Target {target[:, t]}\")\n",
    "\n",
    "                # Get next input with or without teacher forcing (correction)\n",
    "                input = target[:, t] if random.random() < teacher_force_ratio else prediction\n",
    "\n",
    "                # # Decode\n",
    "                # prediction, hidden = self.decoder(input, hidden, context_vec)\n",
    "\n",
    "                # # print(\"prediction\", prediction)\n",
    "                # predictions[:, t] = prediction\n",
    "                # prediction = prediction.argmax(1)\n",
    "                # # print(f\"Predicted sequence {prediction}; Target {target[:, t]}\")\n",
    "                # input = target[:, t] if random.random() < teacher_force_ratio else prediction\n",
    "\n",
    "        # Predict\n",
    "        else:\n",
    "            input = torch.ones((1, 1))  # SOS\n",
    "            prediction = torch.zeros((1, 1))\n",
    "            predictions = []\n",
    "            while prediction.item() != 2:  # EOS\n",
    "                prediction, hidden = self.decoder(input, hidden)\n",
    "                predictions[:, t] = prediction\n",
    "                prediction = prediction.argmax(1)\n",
    "                input = prediction\n",
    "                predictions.append(prediction.item())\n",
    "\n",
    "        return predictions\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, criterion, optimizer, epoch, loss_fun=None, writer=None):\n",
    "    \n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "\n",
    "    for i, batch in enumerate(train_iter):\n",
    "\n",
    "        # Zero so gradients don't accumate\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Move tensors to cuda\n",
    "        source, source_lens = batch.source\n",
    "        target, target_lens = batch.target\n",
    "        source = source.to(device)  # [B,T]\n",
    "        target = target.to(device)  # [B,T]\n",
    "\n",
    "        # Encode and decode sequence\n",
    "        predictions = model(source, target, source_lens, pack_padded=True)\n",
    "\n",
    "        \n",
    "        assert len(predictions) == len(target)  \n",
    "\n",
    "        #         assert len(iterator) == len(source) == len(target)\n",
    "        #         n = len(source)\n",
    "        \n",
    "        if loss_fun is not None:\n",
    "            calc_loss = loss_fun(predictions, target, target_lens)\n",
    "            #               print(\"Calculated loss\", calc_loss.item())\n",
    "            #             print(\"Averaged loss\", calc_loss.item() / n)\n",
    "            loss = calc_loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "        # print(predictions)\n",
    "        # print(target)\n",
    "\n",
    "        # Truncate first SOS prediction and reshape [B, T, H] > [B*T, H]\n",
    "        predictions = predictions[:, 1:, :].reshape(-1, predictions.size(2))\n",
    "        target = target[:, 1:].reshape(-1)\n",
    "        \n",
    "        #         print(\"Torch CE loss\", criterion(predictions, target).item())\n",
    "\n",
    "\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        if i % 1000 == 499:\n",
    "            print(f\"Training loss for epoch\\t {epoch}: {train_loss}\")\n",
    "            print(f\"Averaged over 1000 epochs\\t {train_loss / 500}\")\n",
    "            print(f\"Total steps\\t {epoch * len(iterator) + i} steps\")\n",
    "            if writer:\n",
    "                writer.add_scalar(\"train_loss\", train_loss / 500,\n",
    "                                 epoch * len(iterator) + i)\n",
    "\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def eval(model, iterator, criterion, epoch, loss_fun=None):\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        epoch_loss = 0\n",
    "        for i, batch in enumerate(iterator):\n",
    "            # Move tensors to cuda\n",
    "            source, source_lens = batch.source\n",
    "            target, target_lens = batch.target\n",
    "            source = source.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            predictions = model(source, target, source_lens, pack_padded=True)\n",
    "\n",
    "\n",
    "            if loss_fun is not None:\n",
    "                calc_loss = loss_fun(predictions, target, target_lens)\n",
    "                #                 print(\"Calculated loss\", calc_loss.item())\n",
    "                #             print(\"Averaged loss\", calc_loss.item() / n)\n",
    "                loss = calc_loss\n",
    "                    \n",
    "            # Truncate first SOS prediction\n",
    "            predictions = predictions[:, 1:, :].reshape(-1, predictions.size(2)) \n",
    "            target = target[:, 1:].reshape(-1)\n",
    "            \n",
    "            assert len(predictions) == len(target)  # B * (T - 1)\n",
    "            \n",
    "            # print(f\"Predictions of shape {predictions.size()}\", predictions)\n",
    "            # print(f\"Target of shape {target.size()}\", target)\n",
    "\n",
    "            #             print(\"Torch ce loss\", criterion(predictions, target).item())\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            if i % 1000 == 999:\n",
    "                print(f\"Validation loss for epoch\\t {epoch}: {train_loss}\")\n",
    "                print(f\"Averaged over 1000 epochs\\t {dev_loss / 1000}\")\n",
    "                print(f\"Total steps\\t {epoch * len(iterator) + i} steps\")\n",
    "\n",
    "        return epoch_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def random_eval(model, data, r=2):\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for _ in range(r):\n",
    "\n",
    "            i = random.randrange(0, len(data))\n",
    "            print(f\"random num: {i}\")\n",
    "            source, target = data[i].source, data[i].target\n",
    "\n",
    "            print(\">\", \" \".join(source))\n",
    "            print(\"=\", \" \".join(target))\n",
    "\n",
    "            # Perform preprocessing (indexing, etc.)\n",
    "            source, source_lens = TEXT.process([source])\n",
    "            target, target_lens = TEXT.process([target])\n",
    "            #             print(source)\n",
    "            #             print(target)\n",
    "            #             print(source_lens)\n",
    "            #             print(target_lens)\n",
    "\n",
    "            # Move tensors to cuda\n",
    "            source = source.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            predictions = model(source, target, source_lens, pack_padded=True)\n",
    "\n",
    "            # Truncate first SOS prediction\n",
    "            predictions = predictions[:, 1:, :]\n",
    "            target = target[:, 1:]\n",
    "\n",
    "            # Get predicted sequences [B, T]\n",
    "            predicted_seqs = torch.argmax(predictions, -1)\n",
    "\n",
    "            for i in range(len(predicted_seqs)):\n",
    "                # for t in range(len(predicted_seqs[i])):\n",
    "                print(f\"  {' '.join([TEXT.vocab.itos[t] for t in predicted_seqs[i, :-1]])}\")\n",
    "\n",
    "                #             print(f\"Predictions of shape {predictions.size()}\", predictions)\n",
    "\n",
    "                #             print(f\"Predicted seq of shape {predicted_seqs.size()}\", predicted_seqs)\n",
    "\n",
    "                #             print(f\"Target of shape {target.size()}\", target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrain, set new params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: {'source_dim': 10004, 'embedding_dim': 512, 'hidden_dim': 512, 'layers': 1, 'dropout': 0.5, 'bidirectional_encoder': True, 'bs': 32, 'epochs': 25, 'lr': 0.001}\n"
     ]
    }
   ],
   "source": [
    "# Instantiate model\n",
    "\n",
    "\n",
    "config = dict(\n",
    "    source_dim=len(TEXT.vocab),\n",
    "    embedding_dim=512,\n",
    "    hidden_dim=512,\n",
    "    layers=1,\n",
    "    dropout=.5,\n",
    "    bidirectional_encoder=True,\n",
    "    bs=32,\n",
    "    epochs=25,\n",
    "    lr=.001\n",
    ")\n",
    "print(f\"Config: {config}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from torchtext.data import BucketIterator\n",
    "\n",
    "train_iter, val_iter, test_iter = BucketIterator.splits((train_data, val_data, test_data),\n",
    "                                                        batch_sizes=(\n",
    "                                                            config.get(\"bs\"), config.get(\"bs\"), config.get(\"bs\")),\n",
    "                                                        sort_within_batch=True,\n",
    "                                                        sort_key=lambda x: len(x.source),\n",
    "                                                        device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   2,   13,   37,   15,  154,   60,  598,   14,   26,   44,   88,   99,\n",
       "          42,    4,    5,    0,   47,   15, 1261,  182,   15,   39,   43,    5,\n",
       "           3])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(train_iter))\n",
    "batch.source[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   2,   18,  135,   38, 4131,    0,   14,   72,   49,    0,    0,   20,\n",
       "         309, 3252,    8,    7,   27,   87,   44,   19,    8,    0,    5,  261,\n",
       "           3])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "list(train_iter)[-1].source[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'these'"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "TEXT.vocab.itos[25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  2, 144,   3,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(train_iter)[-1].target[0][0]  # Batch > sents, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "encoder = Encoder(config).to(device)\n",
    "decoder = Decoder(config).to(device)\n",
    "model = Seq2Seq(encoder, decoder).to(device)\n",
    "# model = torch.load(path)\n",
    "\n",
    "# for n, p in model.named_parameters():\n",
    "#     if p.requires_grad:\n",
    "#         print(n, p, p.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Training\n",
    "loop **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/25 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss for epoch\t 1: 462304.74395751953\n",
      "Averaged over 1000 epochs\t 924.609487915039\n",
      "Total steps\t 2933 steps\n",
      "Training loss for epoch\t 1: 1282471.683959961\n",
      "Averaged over 1000 epochs\t 2564.943367919922\n",
      "Total steps\t 3933 steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 1/25 [05:30<2:12:13, 330.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total training loss for epoch 1:\t 2022114.1791687012\n",
      "\n",
      "Total validation loss for epoch 1:\t 206980.54913330078\n",
      "Random prediction from training data\n",
      "random num: 41732\n",
      "> excellent product . anytime i 'm feeling icky , i mix a packet with water and drink - up . i know i 'll be feeling much better soon .\n",
      "= used this product for years\n",
      "  excellent <EOS> <EOS> <EOS> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "random num: 18724\n",
      "> the coffee is wonderful with no bitter taste or after taste . i prefer a cappucino style of coffee with all ingredients pre mixed . this is one of the best that i have tried and i know i will be re - ordering when i am done with my original purchase .\n",
      "= grove square french vanilla cappuccino\n",
      "  great coffee coffee vanilla <EOS> <EOS> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Random prediction from validation data\n",
      "random num: 6511\n",
      "> we have been feeding our son plum organics since he was about 5 months old . it is a wonderfully convenient way to feed your child quality , healthy , organic fruits and vegetables . we love the product and at $ 15.50 for 12 pouches it is the best price we have seen .\n",
      "= amazing product at the best price out there\n",
      "  our <EOS> <pad> <pad> <pad> <EOS> <EOS> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "random num: 2892\n",
      "> fast shipping . the closest i can buy in the kcup to the actual coffee i drink.<a href=\"http://www.amazon.com / gp / product / b001eo5y8y\">green mountain coffee dark magic ( extra bold ) for keurig brewers , 24-count k - cups ( pack of 2)</a >\n",
      "= dark majic\n",
      "  <unk> roast <EOS> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Training loss for epoch\t 2: 367536.52014160156\n",
      "Averaged over 1000 epochs\t 735.0730402832031\n",
      "Total steps\t 5367 steps\n",
      "Training loss for epoch\t 2: 1094801.50100708\n",
      "Averaged over 1000 epochs\t 2189.60300201416\n",
      "Total steps\t 6367 steps\n",
      "\n",
      "Total training loss for epoch 2:\t 1771525.6279907227\n",
      "\n",
      "Total validation loss for epoch 2:\t 198583.14004516602\n",
      "Random prediction from training data\n",
      "random num: 77052\n",
      "> though this product is a little spendy i was pleased with the flavor and that it mixed well with other ingredients . it made homemade ice cream taste much more creamy . good stuff !\n",
      "= very yummy\n",
      "  great product <EOS> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "random num: 62185\n",
      "> my dogs love the kong but the snacks inside ca nt keep thier interest . i ended up just giving the snacks to them .   they did n't like working hard for their snacks .\n",
      "= dog 's loose interest\n",
      "  great treats favorite <EOS> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Random prediction from validation data\n",
      "random num: 5315\n",
      "> i buy this product at kroger and i love it .   it tastes great and is healthy .\n",
      "= i like this product\n",
      "  great product it <EOS> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "random num: 4516\n",
      "> this coffee is an instant but tastes like freshly brewed .   the aroma of good coffee fills my cup and enhances the start to my day . i would purchase this product again .\n",
      "= a starbucks morning .\n",
      "  good coffee <EOS> wake <EOS> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 2/25 [11:00<2:06:35, 330.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss for epoch\t 3: 335778.9792480469\n",
      "Averaged over 1000 epochs\t 671.5579584960938\n",
      "Total steps\t 7801 steps\n",
      "Training loss for epoch\t 3: 1012665.8466491699\n",
      "Averaged over 1000 epochs\t 2025.3316932983398\n",
      "Total steps\t 8801 steps\n",
      "\n",
      "Total training loss for epoch 3:\t 1660491.5991516113\n",
      "\n",
      "Total validation loss for epoch 3:\t 196227.51455688477\n",
      "Random prediction from training data\n",
      "random num: 57790\n",
      "> i like this combination shampoo and conditioner . it leaves my hair soft and clean , plus i like that the conditioner is included in the product since it saves a step . as for the anti - dandruff claim , it seems true and the product smells nice .\n",
      "= works fine\n",
      "  great product <EOS> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "random num: 51877\n",
      "> now this is coffee ! ! strong bold flavor with no bitterness . thank you thank you thank you !\n",
      "= hello lover !\n",
      "  hello ! <EOS> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Random prediction from validation data\n",
      "random num: 7164\n",
      "> i buy dingo treats for several reasons . one , dogs love them , two they do n't leave stains and three the price is reasonable . our siberian husky carries her 's all over the house .\n",
      "= dogs love dingo 's\n",
      "  dogs love them <EOS> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "random num: 8446\n",
      "> these are priced very well compared to purchasing at the grocery store . arrived packed well and wrapped in shrink wrap . the fruit & nut delight are my favorites of the kind bars . they are filling and really delicious ... great texture !\n",
      "= good value\n",
      "  great value <EOS> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 3/25 [16:29<2:00:59, 329.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss for epoch\t 4: 314607.85052490234\n",
      "Averaged over 1000 epochs\t 629.2157010498047\n",
      "Total steps\t 10235 steps\n",
      "Training loss for epoch\t 4: 959618.7181091309\n",
      "Averaged over 1000 epochs\t 1919.2374362182618\n",
      "Total steps\t 11235 steps\n",
      "\n",
      "Total training loss for epoch 4:\t 1570531.037261963\n",
      "\n",
      "Total validation loss for epoch 4:\t 194918.78555297852\n",
      "Random prediction from training data\n",
      "random num: 64334\n",
      "> my wife bakes this in a breadmaker .   loaves come out uniform , kinda dense , but does not tend to fall apart when slicing .   excellent flavor .   as good as any on the shelf for less than 2 $ a loaf .\n",
      "= good bread\n",
      "  good coffee <EOS> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "random num: 35254\n",
      "> these were much thicker than i had expected ( which is a good thing , they 're huge ) . the title \" extra smokey \" is n't kidding . it may be a little too much for some people , but for me it was perfect . i welcome the extra burst of flavor .\n",
      "= thick & smokey\n",
      "  better than lentils <EOS> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Random prediction from validation data\n",
      "random num: 5311\n",
      "> the flavor is very good , the price is in the ballpark , this is a good buy , do n't hesitate on this one\n",
      "= good taste and great price\n",
      "  good price <EOS> good price <EOS> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "random num: 3856\n",
      "> this stuff looks and tastes exactly like sugar . i 've always hated sugar substitutes till i tried this at a friends house . i ordered my own for our coffee at home , also tried it in baking a cake , wonderful ! ! ! we all love it , kids included !\n",
      "= i ca n't believe how great this tastes - just lke sugar\n",
      "  tastes like sugar <EOS> <pad> <pad> <pad> stuff <EOS> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 4/25 [21:58<1:55:21, 329.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss for epoch\t 5: 301833.76416015625\n",
      "Averaged over 1000 epochs\t 603.6675283203125\n",
      "Total steps\t 12669 steps\n",
      "Training loss for epoch\t 5: 919329.4844970703\n",
      "Averaged over 1000 epochs\t 1838.6589689941407\n",
      "Total steps\t 13669 steps\n",
      "\n",
      "Total training loss for epoch 5:\t 1511184.3602294922\n",
      "\n",
      "Total validation loss for epoch 5:\t 198372.2780456543\n",
      "Random prediction from training data\n",
      "random num: 57892\n",
      "> has soy grits and flax in it , not what you expect in chips , and not adequately labeled as such when purchasing . not the usual corn taste , although it has some health benefits . it 's just not what i expected / wanted , and wo n't buy again .\n",
      "= the baked version of these chips is misleading and non - tasty\n",
      "  baked baked version version <EOS> <pad> misleading <EOS> <pad> <pad> - <EOS> <EOS> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "random num: 23992\n",
      "> i 've been buying this product in local stores but over the past 3 - 4 yrs , it 's been hard to find . it can be a bit dry but it 's good chewing exercise ! i do wish that the packages would stop getting smaller and the price going up\n",
      "= great low fat snack\n",
      "  great fat <EOS> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Random prediction from validation data\n",
      "random num: 1158\n",
      "> not nearly as good tasting as the other kcup ciders i 've had . my girlfriend hated it too . they are going to find the garbage can .\n",
      "= bad taste\n",
      "  good <EOS> <EOS> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "random num: 7060\n",
      "> these are top notch nutritional \" snack \" bars .   with only raw natural ingredients you ca n't go wrong . and such few ingredients with no preservatives . cinnamon roll and banana cookie are the best flavors ...\n",
      "= good snackin '\n",
      "  tasty , but healthy <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 5/25 [27:26<1:49:46, 329.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss for epoch\t 6: 294156.65130615234\n",
      "Averaged over 1000 epochs\t 588.3133026123047\n",
      "Total steps\t 15103 steps\n",
      "Training loss for epoch\t 6: 899858.0476379395\n",
      "Averaged over 1000 epochs\t 1799.7160952758788\n",
      "Total steps\t 16103 steps\n",
      "\n",
      "Total training loss for epoch 6:\t 1479526.2750549316\n",
      "\n",
      "Total validation loss for epoch 6:\t 199528.60763549805\n",
      "Random prediction from training data\n",
      "random num: 9709\n",
      "> i use honey stinger energy chews on all my long distance bike rides .   i ride over 100 miles a week and love all the different flavors .\n",
      "= great stuff\n",
      "  great stuff <EOS> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "random num: 33054\n",
      "> great low carb spaghetti . there is virtually no difference in taste from regular pasta and with the timed reorder it comes out cheaper .\n",
      "= just like the real thing\n",
      "  great <EOS> grass <EOS> pasta <EOS> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Random prediction from validation data\n",
      "random num: 3786\n",
      "> this is good , tasty wild rice .   no grit .   it 's great to always have wild rice on hand .   our usual easy use is to follow alton brown 's instructions for baking brown rice , replacing 1/2 cup of brown rice with this wild rice .   wonderful and easy .\n",
      "= great inexpensive option for having good wild rice on hand\n",
      "  good rice rice <EOS> <pad> rice <EOS> rice <EOS> the <EOS> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "random num: 4338\n",
      "> excellent product .   our dogs sure like these - we 've ordered them several times and they always make a hit . the product arrived quickly and was nicely packaged .\n",
      "= dingo mini bones\n",
      "  dingo dental bones <EOS> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 6/25 [32:55<1:44:13, 329.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss for epoch\t 7: 287025.65740966797\n",
      "Averaged over 1000 epochs\t 574.051314819336\n",
      "Total steps\t 17537 steps\n",
      "Training loss for epoch\t 7: 876033.3075256348\n",
      "Averaged over 1000 epochs\t 1752.0666150512695\n",
      "Total steps\t 18537 steps\n",
      "\n",
      "Total training loss for epoch 7:\t 1441676.3963623047\n",
      "\n",
      "Total validation loss for epoch 7:\t 199012.61807250977\n",
      "Random prediction from training data\n",
      "random num: 34051\n",
      "> our favorite red enchilada sauce , but not available in stores in our area . amazon delivers the product quickly and at a very good price . easier than shopping every grocery store in surrounding towns .\n",
      "= best red enchilada sauce\n",
      "  fresh tasting enchilada sauce <EOS> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "random num: 585\n",
      "> briefly : i am a big fan of feline pine in general so i give it a five star rating -- but this is the original formula and does not clump . if you want a clumping cat litter , make sure you buy the clumping version of this product .\n",
      "= this is not the clumping formula ; it does not clump\n",
      "  rip stuff the clumping <EOS> <pad> <EOS> <pad> <pad> not clump in the <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Random prediction from validation data\n",
      "random num: 6308\n",
      "> one dog loves it , the other eats it but does n't goggle it the way he usually does with other foods\n",
      "= dog owner\n",
      "  great loves <EOS> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "random num: 6152\n",
      "> i did n't mind eating these chips but my kids were not fans of them ... even my daughter who is not a picky eater .   some were a bit tough .   we wo n't be buying these again .\n",
      "= good chips but not great\n",
      "  not chips <EOS> <pad> <pad> price <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 7/25 [38:24<1:38:42, 329.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss for epoch\t 8: 279512.66131591797\n",
      "Averaged over 1000 epochs\t 559.0253226318359\n",
      "Total steps\t 19971 steps\n",
      "Training loss for epoch\t 8: 863416.6120910645\n",
      "Averaged over 1000 epochs\t 1726.833224182129\n",
      "Total steps\t 20971 steps\n",
      "\n",
      "Total training loss for epoch 8:\t 1421594.9176940918\n",
      "\n",
      "Total validation loss for epoch 8:\t 201569.20205688477\n",
      "Random prediction from training data\n",
      "random num: 44904\n",
      "> my 4 month of yorkie pups refused to eat this food . they like other blue buffalo products but not this one\n",
      "= my pups would nt eat it\n",
      "  my dog food n't like it <EOS> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "random num: 68911\n",
      "> i love this tea ! it gives me the energy i need to keep going all day long ! ! i will definitely buy this again !\n",
      "= i love yerba mate\n",
      "  love love yerba mate tea <EOS> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Random prediction from validation data\n",
      "random num: 8125\n",
      "> great price per pound . fast shipping in box and double plastic bags .   nice sized pieces . perfect taste . only wish it had been shipped in bucket as illustrated rather than in well - packed box ; surprise storage problem .   probably will order again\n",
      "= crystallized ginger\n",
      "  great pig ears <EOS> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "random num: 1225\n",
      "> i have used this tea for 2 of my 3 pregnancies . it 's a little hard to 1st get used to the taste but now i love it . it helps strengthen the uterus . i started drinking it in my 3rd trimester and drank it for a few weeks postpartum as well .\n",
      "= great tea for pregnancy\n",
      "  great tea pregnancy pregnancy <EOS> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 8/25 [43:52<1:33:11, 328.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss for epoch\t 9: 274207.4629211426\n",
      "Averaged over 1000 epochs\t 548.4149258422851\n",
      "Total steps\t 22405 steps\n",
      "Training loss for epoch\t 9: 845431.3682556152\n",
      "Averaged over 1000 epochs\t 1690.8627365112304\n",
      "Total steps\t 23405 steps\n",
      "\n",
      "Total training loss for epoch 9:\t 1399332.118774414\n",
      "\n",
      "Total validation loss for epoch 9:\t 203324.18753051758\n",
      "Random prediction from training data\n",
      "random num: 14135\n",
      "> we ordered the almond apricot , almond coconut , and the mango macadamia.<br / > we actually liked them in the order listed.<br / > i think we will only order almond apricot again.<br />they were good !\n",
      "= what we tought about kind bars recently purchased .\n",
      "  we we we kind nuts <EOS> <EOS> <pad> <pad> <EOS> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "random num: 59445\n",
      "> honest kitchen rocks ! they have great products and my puppies love it ! i believe the folks at honest kitchen put a whole lot of love in the products they sale and i highly recommend all their products !\n",
      "= great stuff !\n",
      "  doggie puck ! <EOS> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Random prediction from validation data\n",
      "random num: 6732\n",
      "> the chips tasted stale and were so broken up from poor packaging that i could only eat a few from each bag . the bags were crammed into a box that was lightweight , and thus , the problem . i will not be ordering them again .\n",
      "= broken up to small pieces\n",
      "  nasty <EOS> stale <EOS> <pad> bag <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "random num: 2367\n",
      "> poultry meal!!!!!<br />soybean meal!!!<br /><br />what is wrong with this company ? ? now because we want grain free ... they stick those terrible ingredients!<br />google those ingredients for dogs ... have a barf bag near by !\n",
      "= horrific ingredients ! ! ! !\n",
      "  <unk> it ! <EOS> ! <EOS> <EOS> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 9/25 [49:21<1:27:41, 328.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss for epoch\t 10: 273882.4059753418\n",
      "Averaged over 1000 epochs\t 547.7648119506836\n",
      "Total steps\t 24839 steps\n",
      "Training loss for epoch\t 10: 843788.1802368164\n",
      "Averaged over 1000 epochs\t 1687.5763604736328\n",
      "Total steps\t 25839 steps\n",
      "\n",
      "Total training loss for epoch 10:\t 1381430.8834838867\n",
      "\n",
      "Total validation loss for epoch 10:\t 204515.63858032227\n",
      "Random prediction from training data\n",
      "random num: 73617\n",
      "> from the ingredients listed on the bag , you can tell the makers of this product put their full effort into producing the most nutritious food for your cat . also , my cat goes crazy over the flavor . i will stick with this cat food company for ever .\n",
      "= great cat food .\n",
      "  <unk> product description <EOS> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "random num: 73799\n",
      "> wish i would have known these contain trans fats , although its only 1 gram i would have reconsidered this purchase had the nutrition facts been noted on amazon .\n",
      "= muffin maker\n",
      "  msg msg <EOS> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Random prediction from validation data\n",
      "random num: 12\n",
      "> it is crafted just like it is in mexico . but how big is it ? it has scribly icing on its head . i mean come on put a little crafts work into it . and that much money for something that small .\n",
      "= questions and answers\n",
      "  <unk> <EOS> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "random num: 6409\n",
      "> this is one of the best hot dogs made , unfortunatally it is not widely available .   citymade must be off their rocker if they think people are gon na sale hot dogs for such a crazy price .   granted there is 30 + weiners in a package , still a stupid price .\n",
      "= great hot dog , stupid price ! !\n",
      "  canidae - natural - but - <EOS> <pad> <EOS> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 10/25 [54:50<1:22:12, 328.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss for epoch\t 11: 272862.3323059082\n",
      "Averaged over 1000 epochs\t 545.7246646118164\n",
      "Total steps\t 27273 steps\n",
      "Training loss for epoch\t 11: 833522.6767272949\n",
      "Averaged over 1000 epochs\t 1667.0453534545898\n",
      "Total steps\t 28273 steps\n",
      "\n",
      "Total training loss for epoch 11:\t 1378136.5237731934\n",
      "\n",
      "Total validation loss for epoch 11:\t 205237.92361450195\n",
      "Random prediction from training data\n",
      "random num: 71484\n",
      "> this candy is so delicious ! the combination of chewy licorice and creamy fruit / coconut is perfect . it is addictive , though , so pass ' auf !\n",
      "= wunderbar !\n",
      "  yummy candy <EOS> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "random num: 14458\n",
      "> it seems to be fine i like non fat non instant milk to make icings for cakes instead of so much sugar .\n",
      "= review for non fat milk\n",
      "  tastes of me - non - <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Random prediction from validation data\n",
      "random num: 7448\n",
      "> what a kind deal ! you wo n't regret this way of being kind to yourself ! the variety is excellent -- a taste for every bud !\n",
      "= great flavor , good buy\n",
      "  great ! <EOS> great buy ! <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "random num: 8648\n",
      "> works great for me and also help me with my weight .   i do n't like the drink as much but i drink it as needed .\n",
      "= you really go when taken as directed .\n",
      "  great like taken , taken as directed . <EOS> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 11/25 [1:00:19<1:16:43, 328.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss for epoch\t 12: 264595.08364868164\n",
      "Averaged over 1000 epochs\t 529.1901672973632\n",
      "Total steps\t 29707 steps\n",
      "Training loss for epoch\t 12: 822096.5155029297\n",
      "Averaged over 1000 epochs\t 1644.1930310058594\n",
      "Total steps\t 30707 steps\n",
      "\n",
      "Total training loss for epoch 12:\t 1356772.9808044434\n",
      "\n",
      "Total validation loss for epoch 12:\t 206902.78829956055\n",
      "Random prediction from training data\n",
      "random num: 22627\n",
      "> very good quality , and super fast shipping . product was exactly as described .   the delivery service treated the product a bit rough .\n",
      "= good quality\n",
      "  great <EOS> <EOS> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "random num: 50270\n",
      "> the flavor palet for this coffee is deep and rich ... i really enjoyed it and i have to admit , i felt relaxed and not wired : )\n",
      "= deep\n",
      "  deep <EOS> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Random prediction from validation data\n",
      "random num: 8176\n",
      "> buy lavazza and do n't worry about anything else.<br />this is the first time i buy lavazza coffee beans , and i 'm now a lavazza drinker for the next few years .\n",
      "= coffee beans you will love 4ever\n",
      "  rich and <EOS> 'll buy this coffee <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "random num: 5506\n",
      "> wonderful product and best price i 've found . first quality and well before expiration date . if you have bought all - sorts only to pick out the ones with licorice centers - this is the licorice for you .\n",
      "= the best !\n",
      "  great product dates <EOS> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 12/25 [1:05:47<1:11:14, 328.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss for epoch\t 13: 265666.64361572266\n",
      "Averaged over 1000 epochs\t 531.3332872314453\n",
      "Total steps\t 32141 steps\n",
      "Training loss for epoch\t 13: 824557.9021148682\n",
      "Averaged over 1000 epochs\t 1649.1158042297363\n",
      "Total steps\t 33141 steps\n",
      "\n",
      "Total training loss for epoch 13:\t 1356427.772201538\n",
      "\n",
      "Total validation loss for epoch 13:\t 207693.45489501953\n",
      "Random prediction from training data\n",
      "random num: 35679\n",
      "> got the 10 pack for my georgia section hike of the appalachian trail . excellent source of fruits and vegetables . packed well and the packaging made it easy to unscrew and consume right out of the back . like an organic capri sun from back in the day .\n",
      "= great snack\n",
      "  great bears <EOS> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "random num: 64562\n",
      "> we started giving these to our dog to avoid having to have her teeth cleaned at a cost of 4 times human cleaning .   we 'll see if it worked on our next vet appointment .\n",
      "= do these things work\n",
      "  dog not things <EOS> <EOS> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Random prediction from validation data\n",
      "random num: 5736\n",
      "> simply put they had a quality product , they got people used to it and they were loving it , then they cut corners and pulled the old bait and switch . no longer a premium sauce .\n",
      "= used to be good but they changed their recipe and it tastes bad\n",
      "  delicious <EOS> <EOS> <EOS> <pad> glass container <EOS> <pad> <pad> <pad> <pad> like <EOS> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "random num: 2127\n",
      "> after losing weight , my daughter started this formula and she started gaining weight and growing leaps and bounds with almost no gas or other digestive problems .\n",
      "= baby is gaining weight\n",
      "  my formula my weight <EOS> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 13/25 [1:11:16<1:05:45, 328.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss for epoch\t 14: 265987.7755432129\n",
      "Averaged over 1000 epochs\t 531.9755510864258\n",
      "Total steps\t 34575 steps\n",
      "Training loss for epoch\t 14: 816867.4329528809\n",
      "Averaged over 1000 epochs\t 1633.7348659057618\n",
      "Total steps\t 35575 steps\n",
      "\n",
      "Total training loss for epoch 14:\t 1347562.1120910645\n",
      "\n",
      "Total validation loss for epoch 14:\t 207472.49365234375\n",
      "Random prediction from training data\n",
      "random num: 41509\n",
      "> these biscuits are perfect for my 12 week puppy .. crunchy and just the perfect size for a large breed puppy .   she loves them ! !\n",
      "= puppy biscuits\n",
      "  perfect loves these ! <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "random num: 50157\n",
      "> when a product is past its \" best by \" date , that fact should be revealed so we can decide whether to risk it .   these were stale , hard , inedible caramels .\n",
      "= very stale\n",
      "  <unk> dissatisfied <EOS> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Random prediction from validation data\n",
      "random num: 2518\n",
      "> loma linda swiss stake is a perfect hurry up entree that only needs reheating .   it has a substantial meaty texture with a luscious gray .   i like it best served with potatoes , peas , and a salad .\n",
      "= perfect hurry up entree\n",
      "  loma for <EOS> <EOS> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "random num: 3748\n",
      "> that is so unfair . i was a faithful customer now i have to go buy them somewhere else . why amazon ! ! !\n",
      "= why did the price go up for these popchips\n",
      "  why i get price <EOS> walmart ! $ ! <EOS> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 14/25 [1:16:45<1:00:16, 328.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss for epoch\t 15: 265176.8533935547\n",
      "Averaged over 1000 epochs\t 530.3537067871093\n",
      "Total steps\t 37009 steps\n",
      "Training loss for epoch\t 15: 816647.0956726074\n",
      "Averaged over 1000 epochs\t 1633.2941913452148\n",
      "Total steps\t 38009 steps\n",
      "\n",
      "Total training loss for epoch 15:\t 1347115.2790527344\n",
      "\n",
      "Total validation loss for epoch 15:\t 208090.00381469727\n",
      "Random prediction from training data\n",
      "random num: 35361\n",
      "> i drink it everyday and it tastes great ! each box came packed in clear plastic and i feel secure buying it again . good deal for the money .\n",
      "= wonderful teas\n",
      "  teas teas <EOS> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "random num: 29044\n",
      "> i am very satisfied with this purchase.<br />these pistachios are fresh , crunchy and good tasting.<br />they are the best that i have had in a long time.<br />highly recommend to others .\n",
      "= pistachios the best\n",
      "  pistachios <EOS> best <EOS> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Random prediction from validation data\n",
      "random num: 344\n",
      "> these really are amazing chips . do n't be put off by one of the cheeses being blue cheese ... it is so subtle that even blue cheese haters ( like me)will enjoy these .\n",
      "= caution : kettle chips are addictive !\n",
      "  caution : kettle chips are addictive ! <EOS> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "random num: 2170\n",
      "> this product is hard to find for us in stores in ohio , most k cups are french vanilla coffee , not cappuccino , its delicious , will buy again for sure !\n",
      "= great taste\n",
      "  great coffee <EOS> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 15/25 [1:22:14<54:47, 328.76s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss for epoch\t 16: 265479.2759399414\n",
      "Averaged over 1000 epochs\t 530.9585518798829\n",
      "Total steps\t 39443 steps\n",
      "Training loss for epoch\t 16: 812493.794921875\n",
      "Averaged over 1000 epochs\t 1624.98758984375\n",
      "Total steps\t 40443 steps\n",
      "\n",
      "Total training loss for epoch 16:\t 1337327.1709899902\n",
      "\n",
      "Total validation loss for epoch 16:\t 211970.1178894043\n",
      "Random prediction from training data\n",
      "random num: 9114\n",
      "> these cookies are very good .   they have a nice texture because of the oats and they have a great taste .   they are my favorite cookie .   if you like oats , you likely will enjoy these cookies .\n",
      "= these are very good cookies .\n",
      "  these are very good cookies . <EOS> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "random num: 61190\n",
      "> i 'm using this in vegan ice cream , uncooked . the flavor is exotic , but reminds me more of pinesol than pistachios -lol .\n",
      "= using uncooked - does n't remind me of pistachios\n",
      "  using uncooked - s n't remind me <EOS> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Random prediction from validation data\n",
      "random num: 2975\n",
      "> i am on a diet were i ca n't use sugar ; but i like sugar in my tea it takes a little more than sugar to sweeten it but i can say i now can enjoy my tea again ..\n",
      "= a bit of sweet ...\n",
      "  light <EOS> sweet <EOS> <pad> <EOS> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "random num: 7013\n",
      "> wow this juice made me feel great ! i love it ! it helped me to to the bathroom , gave me amazing energy , helped me loose weight , make me feel good and fills me up : ) i hope eveyone tried this stuff , its the best !\n",
      "= magic juice\n",
      "  great <EOS> <EOS> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 16/25 [1:27:44<49:22, 329.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss for epoch\t 17: 264890.0673828125\n",
      "Averaged over 1000 epochs\t 529.780134765625\n",
      "Total steps\t 41877 steps\n",
      "Training loss for epoch\t 17: 813841.2001953125\n",
      "Averaged over 1000 epochs\t 1627.682400390625\n",
      "Total steps\t 42877 steps\n",
      "\n",
      "Total training loss for epoch 17:\t 1343306.849975586\n",
      "\n",
      "Total validation loss for epoch 17:\t 209145.41305541992\n",
      "Random prediction from training data\n",
      "random num: 10573\n",
      "> buying these in large quantities really helps . these are very high quality seeds packaged well and easy to use . way better than buying an ounce or two at the supermarket for six dollars .\n",
      "= great product . great price .\n",
      "  great value <EOS> <EOS> <pad> <pad> <EOS> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "random num: 65976\n",
      "> it is very good , but it lasts for 2 minutes . they say it lasts \" ridiciously long \" , but it only lasts about 2 minutes . go to \" [ ... ] \" .\n",
      "= very flavorful , but it does n't last long\n",
      "  it good <EOS> <pad> not <EOS> <EOS> last <EOS> <EOS> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Random prediction from validation data\n",
      "random num: 3268\n",
      "> my 3 maltese love these .   the product is great . j&b are a wonderful company to work with .   thanks !\n",
      "= great dog treats\n",
      "  great treats treats <EOS> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "random num: 768\n",
      "> i have had all of the various keurigs since they came on the market for home use . thus , i have tried pretty near all of the various k - cups . this is one of my top 5 favorites . love the coconut in it , and the wonderful coffee flavor . yum ! ! ! !\n",
      "= one of my favorites !\n",
      "  best yet <EOS> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 17/25 [1:33:13<43:54, 329.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss for epoch\t 18: 265421.93518066406\n",
      "Averaged over 1000 epochs\t 530.8438703613281\n",
      "Total steps\t 44311 steps\n",
      "Training loss for epoch\t 18: 814051.254699707\n",
      "Averaged over 1000 epochs\t 1628.102509399414\n",
      "Total steps\t 45311 steps\n",
      "\n",
      "Total training loss for epoch 18:\t 1341150.9260559082\n",
      "\n",
      "Total validation loss for epoch 18:\t 210652.9469909668\n",
      "Random prediction from training data\n",
      "random num: 62377\n",
      "> i am allergic to soy lecithin and this type of gum does not have any in it ... they do not have this flavor in the stores near me so i was excited to get it here ...\n",
      "= so glad to have found this gum on here ...\n",
      "  no disappointment i ordered found it <EOS> <pad> <pad> <EOS> <EOS> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "random num: 14556\n",
      "> my dogs are very picky and really love these treats . packaged well to remain fresh .\n",
      "= dog treats\n",
      "  my treats <EOS> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Random prediction from validation data\n",
      "random num: 8249\n",
      "> this is a very excellent mildly - fiery addition to just about anything .   sprinkle it over traditional japanese fare such as udon , omurice , or okonomiyaki - or on such treats as hamburgers , salmon steaks , or blts !   it gives a wee ' kick ' to just about anything savoury .\n",
      "= delicious !\n",
      "  hp <EOS> <EOS> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "random num: 6570\n",
      "> this ginger really adds to our pies and pastries . it is not readily available in the local super markets . the shipment was quick and the price   was good .   try it you ' ' like it .\n",
      "= agreat addition to our spices .\n",
      "  great rocks ! <EOS> <pad> <EOS> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 18/25 [1:38:43<38:25, 329.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss for epoch\t 19: 264120.0835571289\n",
      "Averaged over 1000 epochs\t 528.2401671142578\n",
      "Total steps\t 46745 steps\n",
      "Training loss for epoch\t 19: 808126.6788330078\n",
      "Averaged over 1000 epochs\t 1616.2533576660155\n",
      "Total steps\t 47745 steps\n",
      "\n",
      "Total training loss for epoch 19:\t 1332208.5106201172\n",
      "\n",
      "Total validation loss for epoch 19:\t 211823.0068359375\n",
      "Random prediction from training data\n",
      "random num: 15578\n",
      "> good price for the product , received promptly .   have used this product many times in the past , like it , and will continue to use it .\n",
      "= coconut oil\n",
      "  <unk> oil <EOS> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "random num: 77306\n",
      "> my dog loves em , i trust the \" newman \" brand after all he was a great man , and his family is committed to doing better in the world . and they love there dogs !\n",
      "= i trust this company\n",
      "  great trust company company <EOS> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Random prediction from validation data\n",
      "random num: 3432\n",
      "> i think sorghum flour is one of my favorite gluten free flours . authentic foods have ground these flours very fine and they really make wonderful bread . the taste and texture are just right . another keeper indeed .\n",
      "= superfine , divine , and all mine .\n",
      "  great <EOS> <pad> <EOS> <pad> gluten free <EOS> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "random num: 7783\n",
      "> my friend gave me some of these greenies that were left from her dog .   i gave my dog one and he was sold .   never thought he would eat these .   good breath now and also is a picky eater ... quick delivery and good price .\n",
      "= goodies for dog\n",
      "  greenies dental dogs <EOS> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 19/25 [1:44:12<32:54, 329.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss for epoch\t 20: 261048.52661132812\n",
      "Averaged over 1000 epochs\t 522.0970532226562\n",
      "Total steps\t 49179 steps\n",
      "Training loss for epoch\t 20: 806135.2496337891\n",
      "Averaged over 1000 epochs\t 1612.270499267578\n",
      "Total steps\t 50179 steps\n",
      "\n",
      "Total training loss for epoch 20:\t 1332205.2727966309\n",
      "\n",
      "Total validation loss for epoch 20:\t 213698.14083862305\n",
      "Random prediction from training data\n",
      "random num: 3053\n",
      "> when i opened a can , it spewed a fetid odor . the water literally looked like sewage . avoid this product . it 's made in china and probably contains high levels of toxicity . i 'm glad amazon refunded my money . i 'm just surprised that amazon carried this product in the first place .\n",
      "= do not buy !\n",
      "  not bad bad <EOS> <EOS> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "random num: 19155\n",
      "> while i 'm still getting used to paying even the reduced price of $ 0.50 / cup for home brew , this was a well - balanced , flavorful hazelnut .   very good .\n",
      "= great , balance coffee for a great price .\n",
      "  rip off <EOS> <EOS> <EOS> <pad> <pad> price <EOS> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Random prediction from validation data\n",
      "random num: 8197\n",
      "> i used this on my cake pops , and it felt like i was making a card , so much fun to use and the final result looked lovely . it was some winner pops . girls in the family loved them :)\n",
      "= looks like realy glitter\n",
      "  too cute <EOS> <pad> <EOS> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "random num: 4501\n",
      "> i just tried the apple carrot juice and was stunned at the terrible after taste . it tasted like bad medicine ! ! ! try this juice before giving it   to your kids !\n",
      "= horrible aftertaste -- tastes like medicine\n",
      "  tastes taste ! no taste sugar ! <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 20/25 [1:49:40<27:24, 328.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss for epoch\t 21: 262754.1011199951\n",
      "Averaged over 1000 epochs\t 525.5082022399903\n",
      "Total steps\t 51613 steps\n",
      "Training loss for epoch\t 21: 808081.6541595459\n",
      "Averaged over 1000 epochs\t 1616.1633083190918\n",
      "Total steps\t 52613 steps\n",
      "\n",
      "Total training loss for epoch 21:\t 1331772.07862854\n",
      "\n",
      "Total validation loss for epoch 21:\t 213476.3221435547\n",
      "Random prediction from training data\n",
      "random num: 10528\n",
      "> great flavor without being overpowering like the peach herbal tea is .   we brew it by itself and also mix with ' plain ' black tea for just a hint of peach .\n",
      "= the best of the peach teas\n",
      "  peach peach tea tea peach tea <EOS> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "random num: 7232\n",
      "> i love these !   they have the same flavor as oreos , with a very similar texture .   as with most gluten - free things , they are a tad grainy , but this is easily overlooked ! ! !   definitely something to keep on hand ( and in hand ) .\n",
      "= wow ! ! !\n",
      "  great ! <EOS> ! <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Random prediction from validation data\n",
      "random num: 6871\n",
      "> unlike any other instant oatmeal . taste and texture are at least the same as the longer cooking type , but even better . great with fruit and a tiny drop of maple syrup .\n",
      "= oatmeal\n",
      "  great <EOS> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "random num: 8629\n",
      "> excellent product , very filling , my teenager who eats nothing but pizza loves them ! ! ! ! very aforable and easy to carry with you .\n",
      "= quaker oatmeal cookie\n",
      "  escargot ! mini ! <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 21/25 [1:55:09<21:55, 328.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss for epoch\t 22: 262867.8815612793\n",
      "Averaged over 1000 epochs\t 525.7357631225586\n",
      "Total steps\t 54047 steps\n",
      "Training loss for epoch\t 22: 807756.6115112305\n",
      "Averaged over 1000 epochs\t 1615.513223022461\n",
      "Total steps\t 55047 steps\n",
      "\n",
      "Total training loss for epoch 22:\t 1331040.1036071777\n",
      "\n",
      "Total validation loss for epoch 22:\t 212571.2777709961\n",
      "Random prediction from training data\n",
      "random num: 73852\n",
      "> i use two tablespoons to make 1/2 gallon of sweet iced tea . every batch tastes the same .   great tea , great price , and great convenience .\n",
      "= great iced tea\n",
      "  great combo <EOS> <EOS> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "random num: 31218\n",
      "> this product was absolutely delicious . we finished a whole back in one day ! shipping was very quick and the packaging was easy to open . i will definitely buy more .\n",
      "= delicious\n",
      "  delicious ! <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Random prediction from validation data\n",
      "random num: 1961\n",
      "> bought this for my dad for fathers day . they 're ok , i 've tasted worse ... and better . i have tried the honey ham one . weird , tastes kind of like a hot dog .\n",
      "= so far so good\n",
      "  not far <EOS> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "random num: 5933\n",
      "> i drink at least 1 sugar free rockstar a day , but not for anywhere near this price . i purchased mine at cosco for 28.99 for the same size of a case . that is less than half the price for those who do n't like math :)\n",
      "= good product way overpriced\n",
      "  good sweetener <EOS> to drink <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 22/25 [2:00:38<16:26, 328.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss for epoch\t 23: 265394.7678527832\n",
      "Averaged over 1000 epochs\t 530.7895357055664\n",
      "Total steps\t 56481 steps\n",
      "Training loss for epoch\t 23: 814074.1073608398\n",
      "Averaged over 1000 epochs\t 1628.1482147216798\n",
      "Total steps\t 57481 steps\n",
      "\n",
      "Total training loss for epoch 23:\t 1340823.6637878418\n",
      "\n",
      "Total validation loss for epoch 23:\t 213288.25384521484\n",
      "Random prediction from training data\n",
      "random num: 121\n",
      "> i have been a gluten free mama user for almost 2 years now . this is my favorite blend ! it works perfectly ! rachel is amazing and has a wonderful product !\n",
      "= awesome gluten free mix !\n",
      "  delicious ! <EOS> <EOS> <EOS> <EOS> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "random num: 71536\n",
      "> my dog loves these treats and they help with her aging dog breath .   these arrived fast and fresh . two paws up !\n",
      "= two paws way up\n",
      "  great product up to my dogs <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Random prediction from validation data\n",
      "random num: 3992\n",
      "> these are the best cookies ever . we all loved them . my dad had to take a couple bags home . was lucky enough to get in the warehouse . love ...\n",
      "= omg\n",
      "  best nana <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "random num: 1855\n",
      "> alvita changed their lea licorice tea not the same - but my daughter likes it .   she normally uses tea in her tea but she said she does not use tea in licorice tea - it is sweet enough .\n",
      "= licorice tea\n",
      "  licorice tea <EOS> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 23/25 [2:06:06<10:57, 328.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss for epoch\t 24: 266425.39471435547\n",
      "Averaged over 1000 epochs\t 532.850789428711\n",
      "Total steps\t 58915 steps\n",
      "Training loss for epoch\t 24: 813502.6412963867\n",
      "Averaged over 1000 epochs\t 1627.0052825927735\n",
      "Total steps\t 59915 steps\n",
      "\n",
      "Total training loss for epoch 24:\t 1339692.3770446777\n",
      "\n",
      "Total validation loss for epoch 24:\t 215497.41638183594\n",
      "Random prediction from training data\n",
      "random num: 65805\n",
      "> nice mix .   but would only be truly happy if i could pick out which cups are included in the package .\n",
      "= like coffee\n",
      "  <unk> a <EOS> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "random num: 12709\n",
      "> we really enjoy this rice and purchase it whenever and wherever we can find it . it 's worth the effort for the nice nutty flavor is an enjoyable change from our normal basmati .\n",
      "= great flavor and texture\n",
      "  great rice <EOS> texture <EOS> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Random prediction from validation data\n",
      "random num: 4495\n",
      "> these bones are amazing and my dogs love them . i have two vigorous chewers so these bones are a much better option than regular rawhides or other treats . they are much more expensive at retailers though , so i love being able to save some money by purchasing them online .\n",
      "= my dog 's best friend ! ! !\n",
      "  my dog 's favorite <EOS> <pad> <EOS> <EOS> <EOS> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "random num: 6909\n",
      "> love , love , love this stuff but as all the others said , hate the new ones . lipton made a big mistake in discontinuing the citrus and raspberry tea to go . these prices are outrageous and as much as i love it , will not pay this ! ! ! ! ! ! ! ! ! ! ! !\n",
      "= big mistake lipton\n",
      "  love larabars ! <EOS> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 24/25 [2:11:35<05:28, 328.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss for epoch\t 25: 260733.88711547852\n",
      "Averaged over 1000 epochs\t 521.467774230957\n",
      "Total steps\t 61349 steps\n",
      "Training loss for epoch\t 25: 813175.8175354004\n",
      "Averaged over 1000 epochs\t 1626.3516350708007\n",
      "Total steps\t 62349 steps\n",
      "\n",
      "Total training loss for epoch 25:\t 1342915.4247131348\n",
      "\n",
      "Total validation loss for epoch 25:\t 216692.54846191406\n",
      "Random prediction from training data\n",
      "random num: 65709\n",
      "> you really ca n't go wrong with any of theo 's chocolates , but i 'm partial to this one - i love dark chocolate , and the texture that the almonds add is lovely . to my palate , this is much smoother than many of the other 70%+ dark chocolates .\n",
      "= theo makes amazing chocoloates\n",
      "  oh <EOS> <pad> ! <EOS> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "random num: 24913\n",
      "> i purchased due to one of the review saying this was much better than yoki tea , i feel this tea has a bitter taste and not much ginger , in fact i added ginger , which did not help the taste at all .\n",
      "= stash lemon ginger tea ( caffeine free )\n",
      "  stash lemon ginger tea ( caffeine free ) <EOS> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Random prediction from validation data\n",
      "random num: 2792\n",
      "> this sampler pack of arbuckles gourmet coffee was a great gift .   i tried one of each coffee sample and they are really good .   they have so many different varieties of coffee . the samples they sent were very good , though .\n",
      "= really good coffee\n",
      "  great good coffee <EOS> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "random num: 2850\n",
      "> my dog liked these but you get very little amount of treats for the cost . my only other complaint is that they do break up and can be very powdery in the jar .\n",
      "= not bad ...\n",
      "  great for my <EOS> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [2:17:03<00:00, 328.96s/it]\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# path = \n",
    "writer = SummaryWriter(\"runs/half\")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=1)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "\n",
    "def loss_fun(predictions, target, target_lens, reduction=torch.sum):\n",
    "    \"\"\"\n",
    "\n",
    "        :param predictions: [B, T, C]\n",
    "        :param target: [B, T]\n",
    "        :param target_lens: [B]\n",
    "\n",
    "        :return: loss: [B]\n",
    "\n",
    "    \"\"\"\n",
    "    assert len(predictions) == len(target) == len(target_lens)\n",
    "    \n",
    "    # Create mask\n",
    "    bs, max_len = target.size()\n",
    "    mask = torch.zeros(bs, max_len)\n",
    "    for i in range(bs):\n",
    "        #         print(i, target_lens[i].item(), max_len)\n",
    "        #         print(mask)\n",
    "        mask[i, :target_lens[i].item()] = 1\n",
    "    \n",
    "    # Remove SOS and reshape\n",
    "    predictions = predictions[:, 1:, :].reshape(-1, predictions.size(2)) # [B*(T-1), C]\n",
    "    target = target[:, 1:].reshape(-1) # [B*(T-1)]\n",
    "    mask = mask[:, 1:].reshape(-1)  # [B*(T-1)]\n",
    "    \n",
    "    # Count total sequences (excluding masked positions)\n",
    "    assert len(predictions) == len(target) == len(mask)\n",
    "    #     n = len(predictions)\n",
    "    n = sum(mask).item()\n",
    "    \n",
    "    prob_correct_seqs = torch.gather(predictions, dim=1, index=target.unsqueeze(1)).cpu()\n",
    "    # print(prob_correct_seqs)\n",
    "    \n",
    "    # Calculate NLL (do before mask to avoid inf if zero used for mask)\n",
    "    prob_correct_seqs = prob_correct_seqs\n",
    "    nll = -torch.log(prob_correct_seqs) \n",
    "    #     print(nll)\n",
    "    \n",
    "    # Reduce nll loss across batch filtered with mask\n",
    "    loss = reduction(nll * mask)\n",
    "    #     print(f\"Loss before: {loss}; and averaged loss over {n} samples: {loss / n}\")\n",
    "    \n",
    "    return loss / n\n",
    "    \n",
    "\n",
    "for epoch in tqdm(range(1, (config.get(\"epochs\") + 1))):\n",
    "    train_loss = train(model, train_iter, criterion, optimizer, epoch, loss_fun, writer)\n",
    "    dev_loss = eval(model, val_iter, criterion, epoch, loss_fun)\n",
    "    \n",
    "    print(f\"\\nTotal training loss for epoch {epoch}:\\t {train_loss}\")\n",
    "    print(f\"\\nTotal validation loss for epoch {epoch}:\\t {dev_loss}\")\n",
    "\n",
    "    print(\"Random prediction from training data\")\n",
    "    random_eval(model, train_data, 2)\n",
    "    print(\"Random prediction from validation data\")\n",
    "    random_eval(model, val_data, 2)\n",
    "\n",
    "    path = \"models/model-half.pt\"\n",
    "    torch.save(model, path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (seq2seq)",
   "language": "python",
   "name": "seq2seq"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
