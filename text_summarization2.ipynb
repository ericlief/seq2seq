{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vbdgtN2nSqQg",
    "outputId": "805aef71-e468-498d-be0e-693b68f9d8ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "FPRWUcWaFaig"
   },
   "outputs": [],
   "source": [
    "# !pip install --upgrade torchtext\n",
    "# !pip install --upgrade torch\n",
    "\n",
    "from torch.utils import data\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "from torch import optim\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchtext\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from random import randrange, choice\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eXB9X_eFFdF6",
    "outputId": "e68e5408-4b88-43d6-b0b6-46ffefbfc672"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vcJS-iiRdFw8",
    "outputId": "2b193c24-3975-42fc-81d9-6d8801801075"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-d057213c6eb1>:5: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
      "  pd.set_option(\"display.max_colwidth\", -1)\n"
     ]
    }
   ],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount(\"/content/gdrive\")\n",
    "\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", -1)\n",
    "# path = \"/content/gdrive/My Drive/data/amazon/amazon-reviews-max50-37k.csv\"\n",
    "path = \"data/amazon-reviews-max50-37k.csv\"\n",
    "df = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 293
    },
    "id": "xgDmJLr-kCP7",
    "outputId": "bb18a769-cbe8-47a4-da5c-89835eb1c83d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Since my first trip to Jamaica, I fell in love with the coffee.  The delivery date was right on and we had Blue Mountain coffee on Christmas morning!  For a treat, try drinking it with sweetened condensed milk!</td>\n",
       "      <td>Jamaican all the way!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>this lavish oatmeal in dark chocolate tastes great. the oatmeal is very convenient and quick to make. it tastes great, like cocoa, not exactly like a rich dark chocolate, but still yummy.</td>\n",
       "      <td>yum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I was truly expecting better from Mrs. Dash, but this is nothing more than vinegary glycerin with some unrecognized flavoring. There is absolutely NO oriental flavor whatsoever. Do yourself a favor and go buy some Balsamic vinegar, it tastes the same.</td>\n",
       "      <td>Don't waste your time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>These are my favorite cookies, I have one almost every day. It's hard to believe that they are vegan, they taste much better than most non-vegan cookies!</td>\n",
       "      <td>I love these cookies!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Two of the most yummy, sinful flavors in one healthy package, who knew?  Love it for the late afternoon pick me up when in fade mode! Thank you ZICO!</td>\n",
       "      <td>decadence made healthful</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                        source  \\\n",
       "0  Since my first trip to Jamaica, I fell in love with the coffee.  The delivery date was right on and we had Blue Mountain coffee on Christmas morning!  For a treat, try drinking it with sweetened condensed milk!                                            \n",
       "1  this lavish oatmeal in dark chocolate tastes great. the oatmeal is very convenient and quick to make. it tastes great, like cocoa, not exactly like a rich dark chocolate, but still yummy.                                                                   \n",
       "2  I was truly expecting better from Mrs. Dash, but this is nothing more than vinegary glycerin with some unrecognized flavoring. There is absolutely NO oriental flavor whatsoever. Do yourself a favor and go buy some Balsamic vinegar, it tastes the same.   \n",
       "3  These are my favorite cookies, I have one almost every day. It's hard to believe that they are vegan, they taste much better than most non-vegan cookies!                                                                                                     \n",
       "4  Two of the most yummy, sinful flavors in one healthy package, who knew?  Love it for the late afternoon pick me up when in fade mode! Thank you ZICO!                                                                                                         \n",
       "\n",
       "                     target  \n",
       "0  Jamaican all the way!     \n",
       "1  yum                       \n",
       "2  Don't waste your time     \n",
       "3  I love these cookies!     \n",
       "4  decadence made healthful  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 192
    },
    "id": "6rPAFaLCk7vV",
    "outputId": "224250ce-eeb7-4c5d-dcdc-805253635007"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>37089</td>\n",
       "      <td>37089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>32851</td>\n",
       "      <td>26076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>It was touted by a friend as a sleep inducer, so I tried it and it works.  It tastes kinda mediciney, but it definitely does the job.</td>\n",
       "      <td>Delicious!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>8</td>\n",
       "      <td>207</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                       source  \\\n",
       "count   37089                                                                                                                                   \n",
       "unique  32851                                                                                                                                   \n",
       "top     It was touted by a friend as a sleep inducer, so I tried it and it works.  It tastes kinda mediciney, but it definitely does the job.   \n",
       "freq    8                                                                                                                                       \n",
       "\n",
       "            target  \n",
       "count   37089       \n",
       "unique  26076       \n",
       "top     Delicious!  \n",
       "freq    207         "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3dkP1oqilxaq",
    "outputId": "e8f287e6-5798-4d6a-dea8-08d89b60294d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37089"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "7mhRl7HjuqNV"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liefe/.virtualenvs/s2s/lib/python3.8/site-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
      "/home/liefe/.virtualenvs/s2s/lib/python3.8/site-packages/torchtext/data/example.py:68: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n",
      "/home/liefe/.virtualenvs/s2s/lib/python3.8/site-packages/torchtext/data/example.py:78: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n"
     ]
    }
   ],
   "source": [
    "from torchtext.data import Field, TabularDataset\n",
    "import spacy\n",
    "\n",
    "TEXT = Field(use_vocab=True, init_token=\"<SOS>\", eos_token=\"<EOS>\", \n",
    "               fix_length=25, tokenize=\"spacy\",\n",
    "               include_lengths=True, batch_first=True, lower=True,\n",
    "               is_target=False\n",
    "               )\n",
    "\n",
    "fields = [(\"source\", TEXT),\n",
    "          (\"target\", TEXT)]\n",
    "\n",
    "data = TabularDataset(path, \"CSV\", fields, skip_header=True)\n",
    "                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "1ZFXh5ue5kw-"
   },
   "outputs": [],
   "source": [
    "train_data, test_data = data.split(.70)\n",
    "train_data, val_data = train_data.split(.90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CgYg6bTt_Ush",
    "outputId": "eeae9553-3eda-4296-b468-72d85d58bda7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23366, 2596, 11127)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data), len(val_data), len(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lTR9oYkoOUYI"
   },
   "source": [
    "Build vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "PvsU4PYf6zDx"
   },
   "outputs": [],
   "source": [
    "# TEXT.build_vocab(train_data, vectors=\"fasttext.en.300d\") \n",
    "# TEXT.build_vocab(train_data, vectors=\"fasttext.en.300d\") \n",
    "# TEXT.build_vocab(train_data, vectors=\"glove.6B.100d\", max_size=10000, min_freq=3)\n",
    "TEXT.build_vocab(train_data, max_size=10000, min_freq=3)\n",
    "\n",
    "#  Next time max_size=10000, min_freq=3 ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d0LScDc-8YJN",
    "outputId": "6fb54a7a-f612-442d-a44b-e073b718e946"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens in source/target vocabulary: 8464\n"
     ]
    }
   ],
   "source": [
    "print(f\"Unique tokens in source/target vocabulary: {len(TEXT.vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fW-6xlLl6oi4",
    "outputId": "38c5512e-ca37-4981-85de-e22d8b8db132"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['source', 'target'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[0].__dict__.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BCb8Sir27Oh5",
    "outputId": "33e15d4e-0c13-4f8e-d376-06ab2dc4daca"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['i',\n",
       "  'have',\n",
       "  'yet',\n",
       "  'to',\n",
       "  'find',\n",
       "  'a',\n",
       "  'kashi',\n",
       "  'product',\n",
       "  'that',\n",
       "  'i',\n",
       "  'do',\n",
       "  \"n't\",\n",
       "  'like',\n",
       "  ';',\n",
       "  'and',\n",
       "  'this',\n",
       "  'cereal',\n",
       "  'is',\n",
       "  'especially',\n",
       "  'tasty',\n",
       "  '.',\n",
       "  ' ',\n",
       "  'i',\n",
       "  'had',\n",
       "  'already',\n",
       "  'tried',\n",
       "  'it',\n",
       "  'before',\n",
       "  'ordering',\n",
       "  'from',\n",
       "  'amazon',\n",
       "  ',',\n",
       "  'so',\n",
       "  'i',\n",
       "  'knew',\n",
       "  'it',\n",
       "  'was',\n",
       "  '\"',\n",
       "  'safe',\n",
       "  '\"',\n",
       "  'to',\n",
       "  'order',\n",
       "  'six',\n",
       "  'boxes',\n",
       "  'at',\n",
       "  'a',\n",
       "  'time',\n",
       "  '!',\n",
       "  ' ',\n",
       "  'yummy',\n",
       "  '.'],\n",
       " ['great', 'cereal', '!'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0].source, train_data[0].target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9olpuJPw_9WD",
    "outputId": "b05a258a-9535-467a-bcb2-55dc7b4c8d6a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29761, 20164, 1)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.vocab.freqs[\"the\"], TEXT.vocab.freqs[\"a\"], TEXT.vocab.freqs[\"shit\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4btMuq4UDga4",
    "outputId": "61b46bf4-6750-4b64-d521-ed6d130e2f54"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 'the')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.vocab.stoi[\"the\"], TEXT.vocab.itos[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HeyFiZ784Ufb",
    "outputId": "40994b90-3581-4a71-f424-1d74705a3778"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretty printing has been turned OFF\n",
      "([['<SOS>', 'Eric', 'needs', 'to', 'leave', '<EOS>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']], [6])\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "batch = [\"Eric needs to leave\".split()]\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "pad = TEXT.pad(batch)\n",
    "%pprint\n",
    "print(pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2E38TNNn6uhR",
    "outputId": "d6b95a55-971c-4827-efee-bf5231fb103d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20938\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([['<SOS>', 'i', \"'ve\", 'been', 'doing', 'bonsai', 'for', 'many', 'years', '...', 'even', 'had', 'a', 'business', 'for', 'a', 'few', 'years.<br', '/>this', 'is', 'a', 'nice', 'tree', '.', '<EOS>']], [25])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "i = random.randrange(0, len(train_data))\n",
    "print(i)\n",
    "source = train_data[i].source\n",
    "TEXT.pad([source])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8r46NCT07vo1",
    "outputId": "6327b39b-3f91-4e03-cfe4-2068cfd68e4a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'have', 'yet', 'to', 'find', 'a', 'kashi', 'product', 'that', 'i', 'do', \"n't\", 'like', ';', 'and', 'this', 'cereal', 'is', 'especially', 'tasty', '.', ' ', 'i', 'had', 'already', 'tried', 'it', 'before', 'ordering', 'from', 'amazon', ',', 'so', 'i', 'knew', 'it', 'was', '\"', 'safe', '\"', 'to', 'order', 'six', 'boxes', 'at', 'a', 'time', '!', ' ', 'yummy', '.']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0].source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['great', 'cereal', '!']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0].target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z3OaVsC68dSG",
    "outputId": "b80cfc47-01ab-41c1-c238-43274b51a9bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16117\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[   2,   41,    9, 2097,   38,    0, 6650,    8,   13,   15,   18,   87,\n",
       "          154,   14,   71,  109,  427, 1147,   17,  346,    4,    6,   31,   10,\n",
       "            3]]), tensor([25]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "i = random.randrange(0, len(train_data))\n",
    "print(i)\n",
    "source = train_data[i].source\n",
    "TEXT.process([source])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3OVUqN8cn48u"
   },
   "source": [
    "Define iterator/batcher/loader here note that this is "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2d7wjmGSVQNq"
   },
   "source": [
    "Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "K20QxApIxZ76"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "  def __init__(self, config):\n",
    "    super(Encoder, self).__init__()\n",
    "    self.source_dim = config[\"source_dim\"] # vocab size/number of classes\n",
    "    self.embedding_dim = config[\"embedding_dim\"]\n",
    "    self.hidden_dim = config[\"hidden_dim\"]\n",
    "    self.layers = config.get(\"layers\", 1)\n",
    "    self.bidirectional_encoder = config.get(\"bidirectional_encoder\", False)\n",
    "\n",
    "    # Layers\n",
    "    self.embedding = nn.Embedding(self.source_dim, self.embedding_dim)\n",
    "    self.dropout = nn.Dropout(config.get(\"dropout\", 0.0))\n",
    "    self.gru = nn.GRU(self.embedding_dim, self.hidden_dim, dropout=config.get(\"dropout\", 0.0), num_layers=self.layers, bias=True, batch_first=True, bidirectional=self.bidirectional_encoder)\n",
    "    # self.gru = nn.GRU(64, 64, dropout=0, num_layers=1, bias=True, batch_first=True, bidirectional=False).to(device)\n",
    "\n",
    "  def forward(self, source, source_lens=None, pack_padded=False):\n",
    "    \n",
    "    \"\"\"\n",
    "    :param source: [B, T]\n",
    "    :param source_lengths: [T]\n",
    "    :param pack_padded: bool\n",
    "    :return: output: [B, T, H], hidden: Union([1, B, H] or [2, B, H])\n",
    "    \"\"\"\n",
    "    # print(\"encoder forward\")\n",
    "    # print(input)\n",
    "\n",
    "    # Embed source index sequences [B, T] > [B, T, E]\n",
    "    embedded = self.embedding(source)\n",
    "    \n",
    "    # print(embedded)\n",
    "    \n",
    "    # Apply dropout\n",
    "    embedded = self.dropout(embedded)\n",
    "  \n",
    "    # Pack if padded sequences and lengths given\n",
    "\n",
    "    if pack_padded:\n",
    "        \n",
    "            \n",
    "      embedded = pack_padded_sequence(embedded, source_lens, batch_first=True, enforce_sorted=False)\n",
    "\n",
    "    # Map embedded source [B, T, E] to Union([B, T, H] or [B, T, 2*H], Union([1, B, H] or [2, B, H])\n",
    "    output, hidden = self.gru(embedded) # TODO: add state tuple\n",
    "\n",
    "    if pack_padded:\n",
    "      output, output_lens = pad_packed_sequence(output, batch_first=True)\n",
    "\n",
    "    # Sum outputs of bidirectional RNN: \n",
    "    # [B, T, 2*H] > [B, T, H]\n",
    "    if self.bidirectional_encoder:\n",
    "      output = output[:, :, :self.hidden_dim] + output[:, :, self.hidden_dim:] \n",
    "\n",
    "    return output, hidden \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "nzsXEuFR0k4j"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "  def __init__(self, config):\n",
    "    super(Decoder, self).__init__()\n",
    "    self.source_dim = config[\"source_dim\"] # vocab size/number of classes\n",
    "    self.embedding_dim = config[\"embedding_dim\"] # Can be different from hidden!?\n",
    "    self.bidirectional_encoder = config.get(\"bidirectional_encoder\", False)\n",
    "    self.hidden_dim = config[\"hidden_dim\"]\n",
    "    # self.hidden_dim = 2 * config[\"hidden_dim\"] if self.bidirectional_encoder else config[\"hidden_dim\"]\n",
    "\n",
    "    # Layers\n",
    "    self.embedding = nn.Embedding(self.source_dim, self.embedding_dim)\n",
    "    self.dropout = nn.Dropout(config.get(\"dropout\", 0.0))\n",
    "    self.gru = nn.GRU(self.embedding_dim+self.hidden_dim, self.hidden_dim, dropout=config.get(\"dropout\", 0.0), bias=True, batch_first=True)\n",
    "    self.fc = nn.Linear(self.hidden_dim, self.source_dim) # input classes = output classes\n",
    "    self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    # For attention\n",
    "    self.source_layer = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
    "    self.state_layer = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
    "    self.weight_layer = nn.Linear(self.hidden_dim, 1)\n",
    "\n",
    "  def forward(self, input, hidden, enc_outputs):\n",
    "    \"\"\"\n",
    "    Process batch inputs at one time step and decode prediction.\n",
    "\n",
    "    :param input: [B]\n",
    "    :param hidden: [1, B, H]\n",
    "    :param context_vec: [1, B, H]\n",
    "    :return: prediction (B, H), hidden (1, B, H)\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # print(\"decoder forward\")\n",
    "    # print(input)\n",
    "    \n",
    "    # Process the batch inputs only at this timestep t \n",
    "    # Input of shape [B], but we want [B, 1] \n",
    "    input = input.unsqueeze(1)\n",
    "\n",
    "    # print(f\"input of shape {input.size()}\")\n",
    "    # print(f\"context vec of shape {context_vec.size()}\")\n",
    "\n",
    "\n",
    "    # Embed input [B, 1] > [B, 1, E] \n",
    "    # Note embedding dimension needs to equal hidden if concatenated\n",
    "    # But could map to same space\n",
    "    input = self.embedding(input)\n",
    "    # print(input)\n",
    "\n",
    "    # Apply attention and get next input\n",
    "    # [B, 1, 2*H]\n",
    "    next_input = self.attention(input, hidden, enc_outputs)\n",
    "\n",
    "    \n",
    "    # Sum context vec and input ???padded = field.pad(minibatch)\n",
    "    # embedded = embedded + context_vec \n",
    "\n",
    "    # Apply dropout\n",
    "    # embedded = self.dropout(embedded)\n",
    "\n",
    "\n",
    "\n",
    "    # embedded = pack_padded_sequence(embedded, input_lens, batch_first=True, enforce_sorted=False)\n",
    "\n",
    "    # Feed to RNN\n",
    "    output, hidden = self.gru(next_input, hidden)\n",
    "    \n",
    "    # print(f\"output==hidden {output[0][0]==hidden[0][0]}\")\n",
    "\n",
    "    # output, output_lens = pad_packed_sequence(output, batch_first=True)\n",
    "\n",
    "    # Shape of output: (B, 1, H), we want (B, H)\n",
    "    output = output.squeeze(1)\n",
    "\n",
    "    # Get distribution of classes\n",
    "    output = self.fc(output)\n",
    "\n",
    "    return output, hidden\n",
    "\n",
    "  def attention(self, input, hidden, enc_outputs):\n",
    "\n",
    "\n",
    "    # Calculate attention\n",
    "\n",
    "    # Calculate attention distribution alpha for all encoder outputs\n",
    "\n",
    "\n",
    "    # # Find attention using dot product score (source X hidden_target)\n",
    "    # Didn't work too well\n",
    "    # # Reshape target hidden: [1, B, H] > [B, H, 1]\n",
    "    # h_t = hidden.view(-1, self.hidden_dim, 1)\n",
    "    # # print(f\" s_t: {s_t.size()}\")\n",
    "\n",
    "    # # For batch, perform matrix mult of outputs hidden states and hidden state (s_t) at each time step\n",
    "    # # [B, T, H] X [B, H, 1] = [B, T, 1]\n",
    "    # # print(f\" outputs: {outputs.size()}\")\n",
    "    # score = enc_outputs.bmm(h_t)\n",
    "    \n",
    "    # # Reshape [B, T, 1] > [B, 1, T] and take softmax   \n",
    "    # alphas = self.softmax(score.transpose(1,2))\n",
    "    # # print(f\" alphas: {alphas.size()}\")\n",
    "\n",
    "    # # Find attention. Take weighted sum of hidden states (i.e. linear combo)\n",
    "    # # [B, 1, T] X [B, T, H] = [B, 1, H]\n",
    "    # context_vec = alphas.bmm(enc_outputs)\n",
    "\n",
    "    # Concatenate next input and context\n",
    "    # [B, 1, H] ; [B, 1, H] = [B, 1, 2*H]\n",
    "    # print(f\" context: {context_vec.size()}\")\n",
    "    # print(f\" in: {input.size()}\")\n",
    "\n",
    "\n",
    "\n",
    "    # Find attention using Bahdenah\n",
    "    # tanh(Vh_s + Wh_t + b) \n",
    "\n",
    "    # Project source layer: Vh_s\n",
    "    # [B, T, H] > [B, T, H]\n",
    "    source_layer = self.source_layer(enc_outputs) \n",
    "    \n",
    "    # Reshape target hidden: [1, B, H] > [B, 1, H]\n",
    "    h_t = hidden.view(-1, 1, self.hidden_dim)\n",
    "\n",
    "    # Project target hidden state to layer\n",
    "    # [B, 1, H] > [B, 1, H]\n",
    "    state_layer = self.state_layer(h_t)\n",
    "    \n",
    "    # tanh(Vh_s + Wh_t + b) \n",
    "    # [B, T, H] > [B, T, H]\n",
    "    score = torch.tanh(source_layer + state_layer)\n",
    "\n",
    "    # Project to weight layer with 1 unit\n",
    "    # [B, T, H] > [B, T, 1]\n",
    "    weights = self.weight_layer(score)\n",
    "\n",
    "    # print(weights[0])\n",
    "\n",
    "    weights = self.softmax(weights)\n",
    "\n",
    "    # print(weights[0])\n",
    "    # print(weights.size())\n",
    "    # print(torch.sum(weights, dim=1)[0])\n",
    "\n",
    "    # Find attention. Take weighted sum of hidden states (i.e. linear combo)\n",
    "    # [B, 1, T] X [B, T, H] = [B, 1, H]\n",
    "    context_vec = weights.view(weights.size(0), 1, -1).bmm(enc_outputs)\n",
    "\n",
    "    # print(context_vec.size())\n",
    "    next_input = torch.cat([input, context_vec], dim=-1)\n",
    "\n",
    "\n",
    "    return next_input\n",
    "    # Sum hidden and context_vec\n",
    "    # Didn't seem to work to well\n",
    "    # hidden = hidden + context_vec.view(1, bs, -1)\n",
    "\n",
    "    # Reshape [B, 1, H] > [1, B, H]\n",
    "    # hidden = context_vec.view(1, bs, -1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "je0l4kYWWa-j"
   },
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "\n",
    "  def __init__(self, encoder, decoder):\n",
    "    super(Seq2Seq, self).__init__()\n",
    "    \n",
    "    self.bidirectional_encoder = config.get(\"bidirectional_encoder\", False)\n",
    "    self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    # Block\n",
    "    self.encoder = encoder\n",
    "    self.decoder = decoder\n",
    "\n",
    "  def forward(self, source, target=None, source_lens=None, pack_padded=False, teacher_force_ratio=.5, predict=False):\n",
    "    \"\"\"\n",
    "    :param source: [B, T]\n",
    "    :param target: [B, T] \n",
    "    :param source_lengths: [T]\n",
    "    :param pack_padded: bool\n",
    "    :return: output (B, V), context (B, H), prev_hidden (B, H), weights (B, T)\n",
    "    \"\"\"\n",
    "    \n",
    "    target_vocab_size = self.encoder.source_dim # here same, but may change for MT\n",
    "    hidden_size = self.decoder.hidden_dim  # Pain in ass 2 * encoder hidden size\n",
    "\n",
    "    # Encode, only need output for attention\n",
    "    # out: [B, T, H], for bidirectional torch automatically sums outputs\n",
    "    enc_outputs, hidden = self.encoder(source, source_lens, pack_padded)\n",
    "\n",
    "    # Concatenate hidden states for bidirection encoder\n",
    "    # [2, B, H] > [1, B, 2*H]\n",
    "    # if self.bidirectional_encoder:\n",
    "    #   hidden = torch.cat((hidden[0], hidden[1]), dim=-1)\n",
    "    #   # Add first dim\n",
    "    #   hidden = hidden.unsqueeze(0)\n",
    "\n",
    "    if self.bidirectional_encoder:\n",
    "      hidden = hidden[0] + hidden[1]\n",
    "      # Add first dim\n",
    "      hidden = hidden.unsqueeze(0)\n",
    "\n",
    "    #Iterate through outputs and decode\n",
    "    bs = source.size(0)\n",
    "  \n",
    "    # Train\n",
    "    if not predict:\n",
    "          \n",
    "      # Get start token for batch, e.g. [SOS ...] \n",
    "      target_length = target.size(1) # includes padding (i.e max length) -- to be ignored by loss func\n",
    "      \n",
    "      # [B, 1]\n",
    "      # print(f\" target: {target.size()}\")\n",
    "      input = target[:, 0] # SOS\n",
    "      # print(f\" input: {input.size()}\")\n",
    "\n",
    "      # Get all output predictions step by step and store in matrix of shape [B, T, H]\n",
    "      predictions = torch.zeros(bs, target_length, target_vocab_size).to(device)\n",
    "      for t in range(target_length):\n",
    "        \n",
    "        # Decode\n",
    "        output, hidden = self.decoder(input, hidden, enc_outputs)\n",
    "\n",
    "        # Get predicted output sequences for timestep\n",
    "        prediction = self.softmax(output)\n",
    "        \n",
    "        # print(\"prediction\", prediction)\n",
    "\n",
    "        # [B, H] as softmax distribution\n",
    "        predictions[:, t] = prediction\n",
    "        prediction = prediction.argmax(1)\n",
    "\n",
    "        # print(f\"Predicted sequence {prediction}; Target {target[:, t]}\")\n",
    "        \n",
    "        # Get next input with or without teacher forcing (correction)\n",
    "        input = target[:, t] if random.random() < teacher_force_ratio else prediction\n",
    "\n",
    "\n",
    "\n",
    "        # # Decode\n",
    "        # prediction, hidden = self.decoder(input, hidden, context_vec)\n",
    "\n",
    "        # # print(\"prediction\", prediction)\n",
    "        # predictions[:, t] = prediction\n",
    "        # prediction = prediction.argmax(1)\n",
    "        # # print(f\"Predicted sequence {prediction}; Target {target[:, t]}\")\n",
    "        # input = target[:, t] if random.random() < teacher_force_ratio else prediction\n",
    "\n",
    "    # Predict\n",
    "    else:\n",
    "      input = torch.ones((1,1)) # SOS\n",
    "      prediction = torch.zeros((1,1))\n",
    "      predictions = []\n",
    "      while prediction.item() != 2: # EOS\n",
    "        prediction, hidden = self.decoder(input, hidden)\n",
    "        predictions[:, t] = prediction\n",
    "        prediction = prediction.argmax(1)\n",
    "        input = prediction\n",
    "        predictions.append(prediction.item())\n",
    "        \n",
    "    return predictions \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "I_xyfuXTBLUD"
   },
   "outputs": [],
   "source": [
    "def train(model, iterator, criterion, optimizer):\n",
    "\n",
    "  model.train()\n",
    "  train_loss = 0\n",
    "\n",
    "  for batch in train_iter:\n",
    "\n",
    "    # Zero so gradients don't accumate \n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Move tensors to cuda\n",
    "    source, source_lens = batch.source\n",
    "    target, target_lens = batch.target\n",
    "    source = source.to(device)  # [B,T]\n",
    "    target = target.to(device)  # [B,T]\n",
    "\n",
    "    # Encode and decode sequence\n",
    "    predictions = model(source, target, source_lens, pack_padded=True) \n",
    "    \n",
    "    # Create mask\n",
    "    bs, max_len = target.size() \n",
    "    mask = torch.zeros(bs, max_len)\n",
    "    for i in range(bs):\n",
    "        mask[i, :target_lens[i]] = 1\n",
    "    mask = mask.view(-1)\n",
    "    target = target * masteams\n",
    "    \n",
    "    # Truncate first SOS prediction and reshape [B, T, H] > [B*T, H]\n",
    "    predictions = predictions[:, 1:, :].view(-1, predictions.size(2))\n",
    "    target = target[:, 1:].view(-1)\n",
    "    mask = mask[:, 1:].view(-1)\n",
    "\n",
    "    # print(predictions)\n",
    "    # print(target)\n",
    "\n",
    "    loss = criterion(predictions, target)\n",
    "    \n",
    "    print(f\"Loss before: {loss} and averaged loss over {len(iterator)} samples: {loss/len(iterator)}\")\n",
    "    \n",
    "    loss_ = \n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    train_loss += loss.item()\n",
    "\n",
    "  return train_loss / len(iterator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "RLVTBQm4X4gd"
   },
   "outputs": [],
   "source": [
    "def eval(model, iterator, criterion):\n",
    "\n",
    "  model.eval()\n",
    "\n",
    "  with torch.no_grad():\n",
    "\n",
    "    epoch_loss = 0\n",
    "    for batch in iterator:\n",
    "      \n",
    "      \n",
    "      # Move tensors to cuda\n",
    "      source, source_lens = batch.source\n",
    "      target, target_lens = batch.target\n",
    "      source = source.to(device)\n",
    "      target = target.to(device)\n",
    "      \n",
    "      predictions = model(source, target, source_lens, pack_padded=True)\n",
    "      \n",
    "      # Truncate first SOS prediction\n",
    "      predictions = predictions[:, 1:, :].reshape(-1, predictions.size(2))\n",
    "      target = target[:, 1:].reshape(-1)\n",
    "      # print(f\"Predictions of shape {predictions.size()}\", predictions)\n",
    "      # print(f\"Target of shape {target.size()}\", target)\n",
    "    \n",
    "      loss = criterion(predictions, target)\n",
    "\n",
    "      epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "sEW4SpP8z4Yn"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "    \n",
    "def random_eval(model, data, r=2):\n",
    "\n",
    "  model.eval()\n",
    "\n",
    "  with torch.no_grad():\n",
    "        \n",
    "    for _ in range(r):\n",
    "      \n",
    "      i = random.randrange(0, len(data))\n",
    "      print(f\"random num: {i}\")\n",
    "      source, target = data[i].source, data[i].target\n",
    "\n",
    "      print(\">\", \" \".join(source))\n",
    "      print(\"=\", \" \".join(target))\n",
    "\n",
    "      # Perform preprocessing (indexing, etc.)\n",
    "      source, source_lens = TEXT.process([source])\n",
    "      target, target_lens = TEXT.process([target])\n",
    "      print(source)\n",
    "      print(target)\n",
    "      print(source_lens)\n",
    "      print(target_lens)\n",
    "        \n",
    "        \n",
    "      # Move tensors to cuda\n",
    "      source = source.to(device)\n",
    "      target = target.to(device)\n",
    "      \n",
    "      predictions = model(source, target, source_lens, pack_padded=True)\n",
    "      \n",
    "      # Truncate first SOS prediction\n",
    "      predictions = predictions[:, 1:, :]\n",
    "      target = target[:, 1:]\n",
    "\n",
    "      # Get predicted sequences [B, T]\n",
    "      predicted_seqs = torch.argmax(predictions, -1)\n",
    "\n",
    "      for i in range(len(predicted_seqs)):\n",
    "        # for t in range(len(predicted_seqs[i])):\n",
    "        print(f\"  {' '.join([TEXT.vocab.itos[t] for t in predicted_seqs[i,:-1]])}\")\n",
    "\n",
    "      print(f\"Predictions of shape {predictions.size()}\", predictions)\n",
    "    \n",
    "      print(f\"Predicted seq of shape {predicted_seqs.size()}\", predicted_seqs)\n",
    "\n",
    "      print(f\"Target of shape {target.size()}\", target)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ct8kynYloSQG"
   },
   "source": [
    "## Retrain, set new params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T_sa8nXHjnwX",
    "outputId": "31d4e046-d17e-427b-f2d1-087e2118f5b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: {'source_dim': 8464, 'embedding_dim': 32, 'hidden_dim': 32, 'layers': 1, 'dropout': 0.5, 'bidirectional_encoder': True, 'bs': 32, 'epochs': 1, 'lr': 0.001}\n"
     ]
    }
   ],
   "source": [
    "# Instantiate model\n",
    "\n",
    "\n",
    "config = dict(\n",
    "    source_dim=len(TEXT.vocab),\n",
    "    embedding_dim=512,\n",
    "    hidden_dim=512,\n",
    "    layers=1,\n",
    "    dropout=.5,\n",
    "    bidirectional_encoder=True,\n",
    "    bs=32,\n",
    "    epochs=25,\n",
    "    lr=.001\n",
    ")\n",
    "print(f\"Config: {config}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "-ClhpGLAnj3s"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liefe/.virtualenvs/s2s/lib/python3.8/site-packages/torchtext/data/iterator.py:48: UserWarning: BucketIterator class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "from torchtext.data import BucketIterator\n",
    "\n",
    "\n",
    "train_iter, val_iter, test_iter = BucketIterator.splits((train_data, val_data, test_data),\n",
    "                                                        batch_sizes=(config.get(\"bs\"),config.get(\"bs\"),config.get(\"bs\")),\n",
    "                                                        sort_within_batch=True,\n",
    "                                                        sort_key=lambda x: len(x.source),\n",
    "                                                        device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hSn8x5NWvzQs",
    "outputId": "afc940f8-d5b4-469f-b0ce-38bbb86fcac5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liefe/.virtualenvs/s2s/lib/python3.8/site-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[   2,   18,  859,  153,   13,   74,    6,   29,    9, 1048,    7,   10,\n",
       "           29,  166,   69,    4,   12,  199,   15,   11,   11,   12,   19,   16,\n",
       "            3],\n",
       "        [   2,   13, 3530,   15,  409,  183,  169,    4,   10,  110,   37,   71,\n",
       "          137,   82,    4,  239,   20,  342,    8,  259,   20,  743,   48,  309,\n",
       "            3],\n",
       "        [   2,  769,    5,  235,  219,   30,   51,    5, 5244,  702,    7,   27,\n",
       "           81,   51, 3389,    4,   12,    6,   72,   14,  962,   39,   92,    4,\n",
       "            3],\n",
       "        [   2,   25,  155,   24,    9,  810, 5894,  737,   75,    7,   18,  279,\n",
       "            8,   27,   63,   31,   39,    4,   12,    5,   40, 1550,  336,    8,\n",
       "            3],\n",
       "        [   2,    6,   52,   37,  225,   68,  172,    5,  486,    8,   23,   13,\n",
       "          116,  514,   15, 2612,    4,   12,    5,   90,  391,   58, 1736,   30,\n",
       "            3],\n",
       "        [   2, 5835, 2598,   15,   22,  334,  456,    6,  610,    4,   12,   13,\n",
       "           29,    9,   21,  253,   14,  584,   30,    7,  142,   30,    5, 5781,\n",
       "            3],\n",
       "        [   2,   13,   98,   15,  259,   38,  151,   98,    4,   10,   15,   37,\n",
       "           61,  116,    8,    7,    5,   43,   15, 5598,    4,   10,   15,  129,\n",
       "            3],\n",
       "        [   2,  356,    8, 2154,  616,    6,  225,   23,    6, 1340,  121,   13,\n",
       "          413,   76,  359,    7, 2133,   94,  943,  125,   14,  919,    4,    5,\n",
       "            3],\n",
       "        [   2,    6,   32, 1672,   36,   33,    4,   12,  233,   73,   33,   15,\n",
       "            0, 5509,    6,   78,   13,  527,   49,  384,   14,   54,  890,   41,\n",
       "            3],\n",
       "        [   2,  269,  275,   15,   63,   19,   16,   18,  843,    8,  375,   42,\n",
       "           18,    0,  323,  108,   48,  303,   35,   50,   17,    5, 3646,   29,\n",
       "            3],\n",
       "        [   2, 1369,  215,  481,   24,   19,   30,  744,    8,  425,  401,    8,\n",
       "            7, 5475,    4,   12,    5,  314,   47,   17,    5, 1077,  110,   22,\n",
       "            3],\n",
       "        [   2,    6,   52,   37,  225,   94,   14,  273,   35,  455,   26,   22,\n",
       "         1356,    4,   68,   28,   32,  514,  143,   28,   53,   32,   25,    4,\n",
       "            3],\n",
       "        [   2,   68,   28,   26,    9,   33, 2136,    8,   13,   15,    5,   46,\n",
       "           33,    8,    6,   64,  200,  124,   10,   11,    5,   43,    7,  505,\n",
       "            3],\n",
       "        [   2,   82,   17,    5, 4715,   20, 1498,   38,  563,    8,  533,    8,\n",
       "         1404,  840,   17,    0,    8,   73,  746,    4,   68,   28,  415,    9,\n",
       "            3],\n",
       "        [   2,    6,  178,    9,  538,   17,   25,   16,  518, 1052,    7,   27,\n",
       "           81,    9,  288,  605,    4,   12,    5,  318,   81,  119,    7, 2980,\n",
       "            3],\n",
       "        [   2,    6,  126,    9, 3898,   14,  156,   30,    9,  891,   17,    0,\n",
       "          736,    8, 4350,   39,   97,   16,  518, 5846,    4,    5,    0,   17,\n",
       "            3],\n",
       "        [   2,   62,  234,  469,    5,  617,   17, 1735, 3095,  173,  163,   39,\n",
       "           61, 4005,    7,  777,  992,    4,   12,   26,  270,    0,  246,  211,\n",
       "            3],\n",
       "        [   2,   18, 1541,   16,    9,   19,    7, 1258,  363,   15,  182,    4,\n",
       "           13,  363,   15,  123,   16,  880,    8, 1245,   60,  330,    4,    5,\n",
       "            3],\n",
       "        [   2,  949,  368,    5,   46,  147,   98,   14,  565,   16,    5,  365,\n",
       "         8371,    4,  116,    8,  627,    7,  393,  119,    4,    6,  190,   10,\n",
       "            3],\n",
       "        [   2,   25,   24,  338,  157,   11,   12,   27,   52,   37,  338, 5870,\n",
       "           97,  125,   30,    9,   33,   35,   28,   71,   49,    9,   33,  362,\n",
       "            3],\n",
       "        [   2,   52,   37, 1931,  101,  291,   42,  245,    0,    8,   13,   40,\n",
       "           15,   94,  245,  802,    0,    0,   86,    4,   12,  110,   22,  335,\n",
       "            3],\n",
       "        [   2,   41,  387,    8,   13,   15,    9,   21,  160,   95,   17,   33,\n",
       "            4,  600,    5,  186,   24, 8259,  223,    8,    6,  122,  190,    5,\n",
       "            3],\n",
       "        [   2,   25,  143,   81,   21,    4,    6, 1161,   97,  292,    9,  280,\n",
       "          170,    9,  169,   16,    9,  308, 1225,   27,   81,   44,   21,    4,\n",
       "            3],\n",
       "        [   2,    6,   32,   13,   40,   34,   80,  337,    6, 1281,   10, 1376,\n",
       "         3423,   14,   86,    4,   12,    6,   86,   10,   20, 2405,    8,   23,\n",
       "            3],\n",
       "        [   2,   13,   15,   18,   87,   33,   47,    7,    6,   26,   22,   72,\n",
       "           10,   20,    9,  255,   84,    4,   12, 2580,  252,   69,   10,   29,\n",
       "            3],\n",
       "        [   2,    6,  126,    9,  176,   14, 1158,    7,   26,  403,  286,   14,\n",
       "          758,   14,  197,    9,  315,  767,    4,   12,   13,   15,    9,   69,\n",
       "            3],\n",
       "        [   2,   55,    6,  937,   13,  462,    6,   72,   14,   71,   10,    4,\n",
       "            5,   43,   15,   48,  181,   22,   14,  116,   60,  326,    4,    5,\n",
       "            3],\n",
       "        [   2,  149,   25,  246,   14,   63,  501, 1160,  355, 1959,    7,  244,\n",
       "           19,   43,  231,   29, 2153,   42,    0,    0, 1597,    4, 7121,   29,\n",
       "            3],\n",
       "        [   2,   13,   15,   34,  293,    7,  160,   11,   48,    5,  154,    6,\n",
       "           32,   10,   11,   73,  326,  152,   43,    8,   48,   21,   33,   11,\n",
       "            3],\n",
       "        [   2,  284,   31,   25, 1101, 2295,   11, 1175,  372,  261,   27,  156,\n",
       "           16,  182,  596, 1119,    8,   44,    6,   29, 1090,   74,    6,  149,\n",
       "            3],\n",
       "        [   2,   68,   28,   31,    0, 1141,    8,   13,   15,    5,  559,   27,\n",
       "          863,  241,   11,   80,   82,   66,    5,  767,   58,  361, 4073,    8,\n",
       "            3],\n",
       "        [   2,  721,  163,    5,   46,  914,  928,   45,    6,   26,  112,   72,\n",
       "            4,   12,   10,   62, 4449,   47,    7,  485,  138,  181,   92,   17,\n",
       "            3]]), tensor([25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25,\n",
       "        25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(train_iter))\n",
    "batch.source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xcJdwXx9v0gA",
    "outputId": "efdf4a7e-3a79-44eb-cc43-bd195f61a7f8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[   2,    6,  307, 1132,   58, 2496,    8,   25,  164,   64,   54,  821,\n",
       "            7,   21,    4,   27,   81,   34,  383,    7, 3170,    4,    3,    1,\n",
       "            1],\n",
       "        [   2,    5,  242,   79,   83,   25,  155,    4,   23,   27,   24,  393,\n",
       "         2749,    7,  175,   16,  242,  131,   14,  111,    4,    3,    1,    1,\n",
       "            1],\n",
       "        [   2,  166,  333,   19,    7,   21,  453,   14,   80,   67,  245, 1779,\n",
       "           36,   57,   52,  101,  482,   33,   57,    0,    4,    3,    1,    1,\n",
       "            1],\n",
       "        [   2,   25,   24,   18,   87,  710,  405,  949,  368,  185,   44,   21,\n",
       "          125,   36,   22,   50,   47,   35,   36,  201,   11,    3,    1,    1,\n",
       "            1],\n",
       "        [   2, 3259,   29,  376,    7,    6,  165,  235,  341,  528,  169,    4,\n",
       "           12,    5, 1173, 1515,   15,   69,   11,   11,    3,    1,    1,    1,\n",
       "            1],\n",
       "        [   2,    6,   86,   13,   30,   18, 5401,  570,  602,    4,  564,   38,\n",
       "          393,  260,    7,  627,    8,   73,  993,   11,    3,    1,    1,    1,\n",
       "            1],\n",
       "        [   2,   63,    0,    7,  119,  196,  352, 1288,    8,   60,   61,  160,\n",
       "            4,    0,  116,   61,    4,   31,   10,    4,    3,    1,    1,    1,\n",
       "            1],\n",
       "        [   2,  100,  567,   16,    9,   79,  191,   15, 1189,   14,  782,    4,\n",
       "           12,   63,  537,  217,  320,  473,  575,    4,    3,    1,    1,    1,\n",
       "            1],\n",
       "        [   2,   44,   70,   10,   43,   32, 4646,   14,   75,   23,   63,  333,\n",
       "           20, 4465,   75,   97,    4,  267,  360,    4,    3,    1,    1,    1,\n",
       "            1],\n",
       "        [   2,   58,    5,  138,  371,  308,   10, 1943,    8,   14,    5,   34,\n",
       "          335, 1840,    8,   13,   15,  284, 1190,    4,    3,    1,    1,    1,\n",
       "            1],\n",
       "        [   2,   13,   15,    5,   46,  496, 1447,   45,    6,   26,  149,   11,\n",
       "           11,  124,   10,   14,   51,   18,  500,   70,    3,    1,    1,    1,\n",
       "            1],\n",
       "        [   2,  923,  205,    8,   19,  150,   33,    8,  115,   16,    9,  138,\n",
       "          518,  301,    4,    0,  339,   15,  844,    4,    3,    1,    1,    1,\n",
       "            1],\n",
       "        [   2,   99,   21,   30,  311,  350,    8,   51,   16,    9,   19,   56,\n",
       "           11,   12,   18,  282,   83,   13,  363,    4,    3,    1,    1,    1,\n",
       "            1],\n",
       "        [   2,   68,   28,   32,  160,  388,   45,   35,  495,  105,   30,  238,\n",
       "            7,  134,    8,  247,   13,   15,   10,    4,    3,    1,    1,    1,\n",
       "            1],\n",
       "        [   2,   22,   80, 2887,  261,   23,  199,   34,  119,    8,  114, 1898,\n",
       "           17,   98,    7,   33,    7,  239, 1234,    4,    3,    1,    1,    1,\n",
       "            1],\n",
       "        [   2,   31,   25,   11,   12,   63,   24,   82,   66,   33,  152,  609,\n",
       "            4,   12,  671,  142,   18,   87,   47,   70,    3,    1,    1,    1,\n",
       "            1],\n",
       "        [   2,   13, 1075,   62, 3213,   56,   14,    5, 1184,   10,   15,   73,\n",
       "          502,    9,   21,   65,   16,    5,  291,    4,    3,    1,    1,    1,\n",
       "            1],\n",
       "        [   2,    6,   86,   25,   16,   18, 1027, 1967,    7,  976,   10,   14,\n",
       "          526,  410,    4,    5,  123,   95,    4,    3,    1,    1,    1,    1,\n",
       "            1],\n",
       "        [   2,    5,  131,  452,   39, 5042,  145,  323,   48,  141,   37, 1866,\n",
       "           92,  417,   27,  447,   44,   80,    4,    3,    1,    1,    1,    1,\n",
       "            1],\n",
       "        [   2,   26,  126,   39,   58,   77,  211,    7,  141,   37,  585,    5,\n",
       "           56,    4,   12,   31,    5,   40,    4,    3,    1,    1,    1,    1,\n",
       "            1],\n",
       "        [   2,  376,  444, 1255,  275,  118,  283,   10,   42,  439,  163,   10,\n",
       "         2448,  314,    7,  157,   11,   11,   11,    3,    1,    1,    1,    1,\n",
       "            1],\n",
       "        [   2,   69,    4,   20,  883,  754,   27,   71, 2318,  624,  336,    8,\n",
       "           23, 2420,  715,   27,   24,  310,    4,    3,    1,    1,    1,    1,\n",
       "            1],\n",
       "        [   2,   13,   15,    9,  533,    8, 1439, 5241,  401,    4,   18,   87,\n",
       "          401,  167,   20,    5, 4457, 1578,    4,    3,    1,    1,    1,    1,\n",
       "            1],\n",
       "        [   2,    5,   40,   29,   34,   21,    8,    7,    5,  339,    6,  248,\n",
       "           29,  311,    7,   34,   80, 2064,    4,    3,    1,    1,    1,    1,\n",
       "            1],\n",
       "        [   2,   31,   13,  243,   16,  224,    4,  142,    6,  106,   10,   16,\n",
       "           18,  465,  191,  122,   83,   10,   11,    3,    1,    1,    1,    1,\n",
       "            1],\n",
       "        [   2,    5, 1126,   81,  162,   11,   13,   29,   18,  193,   84,  207,\n",
       "           76,   58,   77,    7,    6,   29,  841,    3,    1,    1,    1,    1,\n",
       "            1],\n",
       "        [   2,    6,   31,   35,   18,  340,   15, 4666,   14,  839, 3373,   30,\n",
       "           25,  136,  435,   35,  130,   83,   11,    3,    1,    1,    1,    1,\n",
       "            1],\n",
       "        [   2,  126,   13,   41,    9,  301,   16,   96,  706,  275,  879,    4,\n",
       "           12,   10,   29,    9,  605,    4,    3,    1,    1,    1,    1,    1,\n",
       "            1],\n",
       "        [   2,   25,    0,   24,  138,   11,   27,   26,    9,  226,   17,   47,\n",
       "            7,  335,    9,  255,   84,   11,    3,    1,    1,    1,    1,    1,\n",
       "            1],\n",
       "        [   2,   29,  154,   14,  147,   16,   18, 1159,    8,  654,    9,  746,\n",
       "            7,  117,    9,    0,   12, 1273,    3,    1,    1,    1,    1,    1,\n",
       "            1],\n",
       "        [   2,   13,   15,   50,   17,   18,   87, 1336,    7,    6,  148,   34,\n",
       "          184,   35,   77, 1467,   10,   11,    3,    1,    1,    1,    1,    1,\n",
       "            1],\n",
       "        [   2,    9,   89,  416,   23,   19,   16,   74,   28,   24, 1150,  276,\n",
       "           32,  266,   60,  143,   11,    3,    1,    1,    1,    1,    1,    1,\n",
       "            1]]), tensor([23, 22, 22, 22, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 20,\n",
       "        20, 20, 20, 20, 20, 20, 20, 20, 20, 19, 19, 19, 19, 18]))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "list(train_iter)[-1].source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "fJwiBkeQv4Ly",
    "outputId": "39bc4e08-9db1-40ce-af36-b8a23abe20e7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'these'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "TEXT.vocab.itos[25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O9y-qRbVv40m",
    "outputId": "6a74295a-4600-4898-8992-29a556fd0989"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[   2,  157,    3,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1],\n",
       "        [   2,   69,   11,    3,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1],\n",
       "        [   2,   46,  489,  112,   11,   11,   11,    3,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1],\n",
       "        [   2,  114,    7,  116,   11,    3,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1],\n",
       "        [   2,   68,   28,   32, 2928, 3332,    8,   13,   53, 1088,    3,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1],\n",
       "        [   2,   63,  119,    3,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1],\n",
       "        [   2,   49,  554,  163,   75,  203,   10,   36,  136,    3,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1],\n",
       "        [   2,   19,  128,    3,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1],\n",
       "        [   2,   19,   40,   11,   11,   11,    3,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1],\n",
       "        [   2,  446,  724,    0,  404,    3,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1],\n",
       "        [   2,  508, 1741,   17, 4123,  676,    3,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1],\n",
       "        [   2, 1116,    5, 3057,   11,   11,   11,   11,    3,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1],\n",
       "        [   2,   31,   13,   11,   11,   11,    3,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1],\n",
       "        [   2,  844,  194,  444,   45,    3,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1],\n",
       "        [   2,  575,  444,  269,  498,    3,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1],\n",
       "        [   2, 2737,   36,   45, 1842, 2436,    8,    0,   45,  278,    3,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1],\n",
       "        [   2,    9,  102,    7,  742,   87,   35, 2210,    5, 1275,   17,   84,\n",
       "            3,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1],\n",
       "        [   2,  376,   16,  549,   33,    3,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1],\n",
       "        [   2,   28,   24,  201,    3,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1],\n",
       "        [   2,   69,   11,    3,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1],\n",
       "        [   2,   19,  150,   33,  285,   21,   47,    3,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1],\n",
       "        [   2,   99,  201,   14,   75,    3,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1],\n",
       "        [   2,  236,   31,   10,    3,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1],\n",
       "        [   2,  116,    3,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1],\n",
       "        [   2, 1012,    3,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1],\n",
       "        [   2,  303,  152,  430,  388, 1933,   11,    3,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1],\n",
       "        [   2,  140,   95, 1405, 1274,    3,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1],\n",
       "        [   2,  157,   98,    3,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1],\n",
       "        [   2,  359,  515,    0,   47,    3,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1],\n",
       "        [   2,    9,   21,  272,   16,   21,   79,   76,    3,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1],\n",
       "        [   2,   44,   21,   11,    3,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1],\n",
       "        [   2, 2192,   17, 1958,    3,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1]]), tensor([ 3,  4,  8,  6, 11,  4, 10,  4,  7,  6,  7,  9,  7,  6,  6, 11, 13,  6,\n",
       "         5,  4,  8,  6,  5,  3,  3,  8,  6,  4,  6,  9,  5,  5]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(train_iter)[-1].target  # Batch > sents, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-easQq7guepz",
    "outputId": "9e4c2f5f-c587-434d-c308-c7037def7e18"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder.embedding.weight Parameter containing:\n",
      "tensor([[ 1.1736, -1.6631, -0.4518,  ...,  0.0690,  1.0551,  0.7782],\n",
      "        [ 1.8077,  1.0078,  0.5353,  ...,  1.5839, -2.2235, -0.2189],\n",
      "        [-1.1051, -0.2164,  0.0673,  ..., -0.7295, -0.1960,  1.5699],\n",
      "        ...,\n",
      "        [ 0.3608,  0.8543,  0.0910,  ..., -0.3660,  0.9582,  1.1644],\n",
      "        [ 1.5009,  0.9670,  1.7449,  ..., -1.2589, -0.6963,  0.3529],\n",
      "        [ 0.9677,  0.8958, -0.3436,  ..., -0.5509, -0.0756, -1.4694]],\n",
      "       requires_grad=True) torch.Size([8464, 32])\n",
      "encoder.gru.weight_ih_l0 Parameter containing:\n",
      "tensor([[-0.0553, -0.0358,  0.0166,  ...,  0.1285,  0.0167,  0.0793],\n",
      "        [-0.0984,  0.0552,  0.0791,  ..., -0.1457, -0.1589,  0.1010],\n",
      "        [ 0.0441,  0.1551, -0.0932,  ..., -0.1481,  0.1551, -0.0915],\n",
      "        ...,\n",
      "        [ 0.1451,  0.1562, -0.1232,  ...,  0.1406,  0.1049,  0.0093],\n",
      "        [ 0.0973, -0.0706,  0.0533,  ..., -0.1684,  0.0906, -0.0953],\n",
      "        [ 0.0487, -0.0044,  0.0564,  ...,  0.1512,  0.1565,  0.1183]],\n",
      "       requires_grad=True) torch.Size([96, 32])\n",
      "encoder.gru.weight_hh_l0 Parameter containing:\n",
      "tensor([[ 0.1689,  0.0810,  0.1661,  ..., -0.0946,  0.0080,  0.0104],\n",
      "        [-0.0955,  0.0316, -0.0002,  ...,  0.0174,  0.0130,  0.0668],\n",
      "        [ 0.1478, -0.1029,  0.1414,  ..., -0.0110, -0.1718,  0.0690],\n",
      "        ...,\n",
      "        [ 0.0028, -0.1460, -0.1088,  ...,  0.1410,  0.0082, -0.1474],\n",
      "        [-0.1591,  0.0489, -0.1388,  ...,  0.1531,  0.1602,  0.0023],\n",
      "        [ 0.1513, -0.1525, -0.0972,  ...,  0.1030,  0.0481, -0.0414]],\n",
      "       requires_grad=True) torch.Size([96, 32])\n",
      "encoder.gru.bias_ih_l0 Parameter containing:\n",
      "tensor([ 0.1163,  0.0541,  0.1085, -0.1391, -0.0198,  0.0644,  0.1647,  0.0509,\n",
      "        -0.0392,  0.0970, -0.1230, -0.1575, -0.0324,  0.0611, -0.0108,  0.1026,\n",
      "         0.0040, -0.0004,  0.1705, -0.0568, -0.0667,  0.1318,  0.1163, -0.1592,\n",
      "         0.0663,  0.0191, -0.0925,  0.1595, -0.0326,  0.0325,  0.0771, -0.0412,\n",
      "        -0.0700,  0.0295, -0.0608, -0.0620, -0.0639,  0.1398, -0.1713,  0.1128,\n",
      "        -0.0929,  0.1724, -0.0915,  0.0871,  0.1643, -0.1624,  0.0847, -0.1600,\n",
      "        -0.1155,  0.0159,  0.0441,  0.0583,  0.0590, -0.1710, -0.0768, -0.0179,\n",
      "         0.0480, -0.1719,  0.0781,  0.0222, -0.0502, -0.0801,  0.0766, -0.0007,\n",
      "         0.0276, -0.1477, -0.0585, -0.0278,  0.0266, -0.0845, -0.0853,  0.0668,\n",
      "         0.0757,  0.1685,  0.1294,  0.0804,  0.0192, -0.0265,  0.1451, -0.0961,\n",
      "        -0.0877, -0.1354,  0.0184, -0.0559, -0.1168, -0.0039,  0.1148, -0.0080,\n",
      "        -0.0965, -0.0740,  0.1479,  0.1356, -0.0275,  0.1470, -0.0470,  0.0736],\n",
      "       requires_grad=True) torch.Size([96])\n",
      "encoder.gru.bias_hh_l0 Parameter containing:\n",
      "tensor([ 8.7425e-02, -1.7025e-01,  1.5549e-01,  2.8457e-03,  1.2109e-01,\n",
      "         1.6210e-01, -1.6701e-01,  9.3895e-03, -1.0467e-04, -5.8260e-02,\n",
      "         1.1188e-01,  3.3068e-02, -1.0236e-01,  9.4951e-02,  8.2461e-02,\n",
      "         5.9630e-02, -8.4971e-02,  1.6987e-01,  5.4758e-02, -1.5125e-01,\n",
      "         6.7642e-02, -4.4758e-02, -1.6288e-01,  1.6481e-01, -4.5400e-02,\n",
      "         1.5805e-01, -5.5162e-03,  1.0949e-01, -1.0833e-01, -1.4462e-01,\n",
      "        -1.7392e-01,  6.5359e-02, -6.6701e-02, -1.2581e-01, -6.8135e-02,\n",
      "         1.1436e-01,  1.4716e-01, -9.6232e-02, -1.6595e-01, -8.4550e-02,\n",
      "        -3.1319e-02, -1.5486e-01, -1.5892e-01, -1.1639e-01,  1.6027e-01,\n",
      "         1.6589e-01, -1.0632e-01,  7.9515e-02,  4.4628e-02, -1.3841e-01,\n",
      "        -1.2148e-01,  1.5453e-01,  1.0148e-01,  1.5404e-01, -2.7187e-02,\n",
      "         1.3816e-01, -1.3794e-01, -3.2496e-03,  7.4831e-02, -1.2859e-02,\n",
      "         1.0246e-01,  8.1828e-02, -2.2002e-02, -6.2303e-03,  1.9521e-02,\n",
      "         3.0944e-02, -8.4940e-02, -2.8485e-02,  1.5897e-01,  1.3716e-01,\n",
      "         1.6266e-01, -9.3318e-02,  1.6283e-01, -8.3114e-02,  1.3093e-01,\n",
      "        -8.0619e-02,  1.5534e-01,  2.4838e-03, -1.0877e-01, -1.1098e-01,\n",
      "        -4.7469e-02,  1.1627e-01, -3.0768e-02, -1.3495e-01,  2.0637e-02,\n",
      "        -1.2911e-01,  1.2753e-02, -1.4553e-01,  3.6284e-02,  1.3760e-01,\n",
      "        -6.5947e-04, -1.6161e-01,  1.4646e-01,  4.8634e-02, -1.5010e-01,\n",
      "         1.1932e-01], requires_grad=True) torch.Size([96])\n",
      "encoder.gru.weight_ih_l0_reverse Parameter containing:\n",
      "tensor([[-0.0163, -0.0315, -0.1483,  ...,  0.1396, -0.1073,  0.1615],\n",
      "        [ 0.0493,  0.0208,  0.1137,  ...,  0.0843, -0.1317, -0.1210],\n",
      "        [-0.1161,  0.1103, -0.0456,  ...,  0.1001, -0.0306, -0.0296],\n",
      "        ...,\n",
      "        [-0.1416, -0.0354,  0.0391,  ...,  0.0361,  0.1286,  0.0995],\n",
      "        [ 0.1625,  0.0698, -0.0035,  ...,  0.1526, -0.0719,  0.1067],\n",
      "        [ 0.0892,  0.0211,  0.0516,  ...,  0.0411,  0.0011, -0.0701]],\n",
      "       requires_grad=True) torch.Size([96, 32])\n",
      "encoder.gru.weight_hh_l0_reverse Parameter containing:\n",
      "tensor([[ 0.1211,  0.1704,  0.1124,  ...,  0.0688,  0.0314, -0.0394],\n",
      "        [-0.1743,  0.1143,  0.0078,  ..., -0.1034, -0.1016,  0.0143],\n",
      "        [-0.0322,  0.0372, -0.0233,  ..., -0.0626, -0.0966, -0.1156],\n",
      "        ...,\n",
      "        [-0.0570,  0.0229, -0.0949,  ...,  0.1496, -0.0186,  0.0164],\n",
      "        [ 0.1666, -0.1183,  0.0508,  ..., -0.1346, -0.1395, -0.0758],\n",
      "        [ 0.0953,  0.0233,  0.1430,  ...,  0.0647,  0.1702, -0.0682]],\n",
      "       requires_grad=True) torch.Size([96, 32])\n",
      "encoder.gru.bias_ih_l0_reverse Parameter containing:\n",
      "tensor([ 0.0854, -0.1739,  0.1616,  0.1345,  0.1594,  0.1512,  0.0218,  0.1766,\n",
      "        -0.1712,  0.1209,  0.1295, -0.1284, -0.1514,  0.0590, -0.0931,  0.1713,\n",
      "        -0.1050,  0.0301, -0.1711,  0.1292, -0.0736,  0.0710, -0.1182,  0.0692,\n",
      "        -0.1234, -0.0999,  0.0703, -0.0494,  0.1712, -0.1409,  0.1294,  0.0344,\n",
      "         0.0254, -0.0514, -0.0885,  0.1321, -0.0904, -0.1473, -0.0449,  0.1143,\n",
      "        -0.0291, -0.0763,  0.0144,  0.1757, -0.0288,  0.1387,  0.1216,  0.1021,\n",
      "        -0.1185, -0.1400, -0.1366,  0.0448, -0.1070,  0.1681,  0.0340, -0.0453,\n",
      "        -0.0820, -0.1593,  0.1257,  0.1653,  0.0194,  0.1085, -0.1516, -0.0288,\n",
      "        -0.0330,  0.1219,  0.1610, -0.0578,  0.1153,  0.1488,  0.0869,  0.1550,\n",
      "         0.1199, -0.0588,  0.0359,  0.1147, -0.1071, -0.1387, -0.0144, -0.0511,\n",
      "        -0.0983, -0.0911, -0.1545, -0.1620,  0.0232,  0.0499, -0.1594,  0.1015,\n",
      "        -0.0838, -0.0537,  0.1331,  0.1097, -0.1298,  0.0304, -0.0994,  0.0885],\n",
      "       requires_grad=True) torch.Size([96])\n",
      "encoder.gru.bias_hh_l0_reverse Parameter containing:\n",
      "tensor([ 0.0824,  0.0481, -0.0603, -0.0864,  0.0664,  0.0686,  0.1523, -0.0463,\n",
      "        -0.0124, -0.1469,  0.0827,  0.0640,  0.0125, -0.1114, -0.0611,  0.1361,\n",
      "         0.1656, -0.0593,  0.0578,  0.0066,  0.0337, -0.1418,  0.1730, -0.1076,\n",
      "         0.1497,  0.1240,  0.0984,  0.1042,  0.0782, -0.0107,  0.1280, -0.1612,\n",
      "         0.1756,  0.0628,  0.0617, -0.1650, -0.1133, -0.1017, -0.1260, -0.0485,\n",
      "         0.0260,  0.0527, -0.0103, -0.1150, -0.1762, -0.0490,  0.0452, -0.1630,\n",
      "        -0.0957,  0.0004, -0.1445, -0.1240, -0.1568,  0.0629,  0.1111, -0.1611,\n",
      "         0.0379, -0.0429,  0.1070,  0.1180,  0.0390,  0.1679, -0.0706, -0.0392,\n",
      "        -0.0694, -0.1518, -0.0146, -0.1442, -0.0429,  0.0022,  0.1577,  0.0206,\n",
      "         0.0066,  0.1410, -0.0888, -0.1595,  0.0012,  0.0562,  0.1574, -0.0582,\n",
      "         0.1260,  0.0529,  0.0976, -0.0317,  0.1707, -0.1085,  0.0893, -0.1198,\n",
      "         0.0275,  0.1113,  0.1396,  0.0026, -0.0830, -0.0299, -0.0822,  0.0570],\n",
      "       requires_grad=True) torch.Size([96])\n",
      "decoder.embedding.weight Parameter containing:\n",
      "tensor([[-1.1725,  0.2565,  0.7564,  ...,  0.8435, -1.8585,  1.3634],\n",
      "        [-0.3523,  2.0757,  1.1289,  ..., -2.6805,  1.0589, -1.5626],\n",
      "        [ 1.1519, -1.4971,  0.6817,  ..., -0.1503, -0.1749,  0.8105],\n",
      "        ...,\n",
      "        [-0.3182, -0.3402,  0.8216,  ..., -0.8893, -0.7710,  0.5785],\n",
      "        [-1.1248, -0.3376, -1.6113,  ..., -0.2269, -0.1386,  0.9306],\n",
      "        [ 0.4036, -0.7850, -0.6842,  ...,  1.4302,  1.2332,  0.9659]],\n",
      "       requires_grad=True) torch.Size([8464, 32])\n",
      "decoder.gru.weight_ih_l0 Parameter containing:\n",
      "tensor([[ 0.1720,  0.0165, -0.0693,  ..., -0.0296, -0.0229, -0.1192],\n",
      "        [-0.1283, -0.0367,  0.0930,  ..., -0.1623, -0.1746, -0.1531],\n",
      "        [ 0.1753, -0.0251, -0.0707,  ...,  0.0415,  0.0757, -0.0028],\n",
      "        ...,\n",
      "        [-0.0889,  0.1099,  0.0586,  ..., -0.1623, -0.0857, -0.0724],\n",
      "        [ 0.1463,  0.0138, -0.1715,  ..., -0.1111, -0.1365, -0.1196],\n",
      "        [ 0.1618, -0.0574, -0.1337,  ..., -0.0112,  0.1656, -0.0748]],\n",
      "       requires_grad=True) torch.Size([96, 64])\n",
      "decoder.gru.weight_hh_l0 Parameter containing:\n",
      "tensor([[-0.0876,  0.0085, -0.1767,  ..., -0.1182, -0.1540,  0.1596],\n",
      "        [ 0.0034,  0.1329, -0.0902,  ...,  0.1622,  0.1260, -0.0609],\n",
      "        [ 0.1078,  0.0985,  0.1636,  ..., -0.0506, -0.1345, -0.1544],\n",
      "        ...,\n",
      "        [ 0.1495,  0.1736,  0.1686,  ...,  0.0349,  0.1089,  0.0248],\n",
      "        [-0.0805,  0.1127, -0.0775,  ..., -0.1152, -0.0765, -0.0928],\n",
      "        [-0.0324, -0.1711,  0.0845,  ...,  0.1121,  0.0642, -0.1249]],\n",
      "       requires_grad=True) torch.Size([96, 32])\n",
      "decoder.gru.bias_ih_l0 Parameter containing:\n",
      "tensor([-0.1637,  0.1734, -0.0420,  0.1714,  0.0853, -0.0743, -0.0608,  0.1558,\n",
      "         0.1573, -0.1751,  0.1419,  0.0765,  0.0536, -0.1584, -0.0889, -0.1281,\n",
      "        -0.1748, -0.1220,  0.0663,  0.1090, -0.0441, -0.1023, -0.0446, -0.0553,\n",
      "        -0.1229,  0.0859,  0.0056,  0.0728, -0.0802, -0.1239,  0.0476,  0.0504,\n",
      "         0.0501,  0.1415,  0.0817,  0.1512,  0.0729,  0.0560, -0.1011,  0.1142,\n",
      "         0.1563,  0.0691,  0.1714, -0.0019, -0.1378, -0.0404, -0.0266,  0.0031,\n",
      "         0.0474,  0.1747,  0.0143, -0.1278,  0.0762,  0.1717, -0.1022,  0.0883,\n",
      "         0.0728, -0.1005, -0.1497,  0.0465,  0.1530,  0.1217,  0.0037, -0.0217,\n",
      "        -0.0386, -0.1286,  0.0657, -0.0841,  0.1676, -0.1326,  0.1216, -0.0800,\n",
      "         0.0505,  0.0396, -0.0734,  0.0463, -0.0448, -0.1065,  0.0012,  0.0718,\n",
      "         0.0439,  0.0399,  0.1214, -0.0742,  0.1492, -0.0824,  0.0419,  0.0125,\n",
      "        -0.0095,  0.1682, -0.0347,  0.1553,  0.0970,  0.1097, -0.1673,  0.0903],\n",
      "       requires_grad=True) torch.Size([96])\n",
      "decoder.gru.bias_hh_l0 Parameter containing:\n",
      "tensor([ 0.0563,  0.1458, -0.0979, -0.0603, -0.0464,  0.1754, -0.0172,  0.1008,\n",
      "         0.1113,  0.1610, -0.0872, -0.1539,  0.0764,  0.1319, -0.1428, -0.0222,\n",
      "        -0.0209, -0.0309,  0.0409, -0.1295, -0.1269,  0.0489, -0.0935,  0.0661,\n",
      "         0.0425, -0.0600,  0.1738, -0.1669, -0.0811,  0.0008,  0.0900, -0.0159,\n",
      "        -0.1725,  0.0760, -0.0821,  0.1084, -0.0993,  0.0178, -0.0722,  0.0227,\n",
      "        -0.1492, -0.0605, -0.1503, -0.1579, -0.0945,  0.0653, -0.0642,  0.0478,\n",
      "        -0.1216, -0.1168,  0.1591, -0.0568,  0.0361,  0.1613,  0.1128,  0.1221,\n",
      "         0.0775,  0.1517, -0.0798,  0.0519, -0.1205,  0.1286, -0.0266,  0.1453,\n",
      "        -0.1614,  0.1712,  0.1373,  0.0322,  0.1414, -0.1682, -0.0480,  0.0191,\n",
      "         0.0859,  0.0830, -0.0698,  0.0067,  0.0133, -0.1751, -0.1168,  0.0059,\n",
      "         0.1651, -0.1715,  0.0990, -0.0376, -0.1588,  0.0911,  0.1406, -0.1155,\n",
      "        -0.0176, -0.0880,  0.1218,  0.1680,  0.0178,  0.0406, -0.1198,  0.1392],\n",
      "       requires_grad=True) torch.Size([96])\n",
      "decoder.fc.weight Parameter containing:\n",
      "tensor([[-0.0212,  0.0193,  0.1663,  ..., -0.1113, -0.1702,  0.0006],\n",
      "        [-0.0687,  0.1712,  0.0310,  ...,  0.0487,  0.0194,  0.0257],\n",
      "        [-0.0814, -0.0956,  0.0398,  ..., -0.0544, -0.1497, -0.1431],\n",
      "        ...,\n",
      "        [ 0.1040, -0.0515, -0.1424,  ..., -0.1450,  0.0638, -0.0117],\n",
      "        [ 0.0774,  0.0606,  0.0989,  ...,  0.0182, -0.1174, -0.1741],\n",
      "        [ 0.1718,  0.0736,  0.0639,  ...,  0.0841, -0.0850, -0.1110]],\n",
      "       requires_grad=True) torch.Size([8464, 32])\n",
      "decoder.fc.bias Parameter containing:\n",
      "tensor([-0.0695, -0.0180, -0.0044,  ...,  0.1065, -0.1125,  0.1221],\n",
      "       requires_grad=True) torch.Size([8464])\n",
      "decoder.source_layer.weight Parameter containing:\n",
      "tensor([[ 0.1646,  0.1115,  0.1524,  ..., -0.0843, -0.0044, -0.0192],\n",
      "        [ 0.1560, -0.0381, -0.1506,  ...,  0.1070, -0.1580,  0.1679],\n",
      "        [-0.0667,  0.1518,  0.0114,  ...,  0.1191, -0.0734, -0.0821],\n",
      "        ...,\n",
      "        [-0.0046, -0.0031,  0.0677,  ...,  0.0255, -0.1236, -0.0098],\n",
      "        [ 0.1251,  0.0595, -0.1652,  ..., -0.0005, -0.1258,  0.1674],\n",
      "        [-0.1121,  0.0282,  0.0144,  ...,  0.0468,  0.0872, -0.0662]],\n",
      "       requires_grad=True) torch.Size([32, 32])\n",
      "decoder.source_layer.bias Parameter containing:\n",
      "tensor([ 0.1291,  0.0169,  0.0999,  0.0081,  0.0433,  0.1375, -0.1552, -0.1210,\n",
      "         0.1426, -0.0548, -0.0638,  0.0838, -0.1505,  0.0843,  0.0145, -0.0237,\n",
      "        -0.0067, -0.0185,  0.1057,  0.1641, -0.1088, -0.1017, -0.1698,  0.1458,\n",
      "         0.0849, -0.1180, -0.0837, -0.1313, -0.0022,  0.1280,  0.0017,  0.1232],\n",
      "       requires_grad=True) torch.Size([32])\n",
      "decoder.state_layer.weight Parameter containing:\n",
      "tensor([[ 0.1192, -0.0919,  0.1409,  ..., -0.1342, -0.0965, -0.0034],\n",
      "        [-0.1273, -0.0345,  0.0987,  ..., -0.1048, -0.1701,  0.0323],\n",
      "        [ 0.1121,  0.1065, -0.1603,  ..., -0.1186, -0.1119, -0.0838],\n",
      "        ...,\n",
      "        [-0.0645, -0.1707,  0.1699,  ...,  0.0957, -0.1595,  0.1577],\n",
      "        [ 0.1100, -0.0873,  0.0745,  ..., -0.0341, -0.0419, -0.1456],\n",
      "        [-0.0358, -0.0236, -0.0145,  ...,  0.0161,  0.1617,  0.0753]],\n",
      "       requires_grad=True) torch.Size([32, 32])\n",
      "decoder.state_layer.bias Parameter containing:\n",
      "tensor([-0.0661,  0.0040, -0.1155,  0.0035,  0.0868, -0.1223,  0.1336, -0.0808,\n",
      "        -0.1551,  0.0847, -0.0758, -0.1570,  0.0034,  0.0307, -0.1274, -0.1501,\n",
      "        -0.1488,  0.1719, -0.0387, -0.0229, -0.0813,  0.1274, -0.0090, -0.1366,\n",
      "         0.1674, -0.0634, -0.0423,  0.0447,  0.0335, -0.1251,  0.0133,  0.0017],\n",
      "       requires_grad=True) torch.Size([32])\n",
      "decoder.weight_layer.weight Parameter containing:\n",
      "tensor([[ 0.0666, -0.1213,  0.0559,  0.0729, -0.0379,  0.1483,  0.1234, -0.0627,\n",
      "          0.1397,  0.0596, -0.0989, -0.0865,  0.1135,  0.1098, -0.1546, -0.0646,\n",
      "         -0.0892, -0.1239, -0.0064, -0.1338, -0.0623, -0.1507, -0.1269, -0.0463,\n",
      "          0.0563,  0.1740, -0.0781,  0.1445,  0.1316,  0.0796,  0.0741, -0.1562]],\n",
      "       requires_grad=True) torch.Size([1, 32])\n",
      "decoder.weight_layer.bias Parameter containing:\n",
      "tensor([0.1179], requires_grad=True) torch.Size([1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liefe/.virtualenvs/s2s/lib/python3.8/site-packages/torch/nn/modules/rnn.py:58: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(config).to(device)\n",
    "decoder = Decoder(config).to(device)\n",
    "model = Seq2Seq(encoder, decoder).to(device) \n",
    "# model = torch.load(path)\n",
    "\n",
    "for n, p in model.named_parameters():\n",
    "  if p.requires_grad:\n",
    "    print(n, p, p.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training loop**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KMjoowAvpwOs",
    "outputId": "73699146-06bb-4331-e806-9806e060ca9e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [04:20<00:00, 260.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training loss for epoch 1:\t 8.862576512077112\n",
      "\n",
      "Dev loss for epoch 1:\t 8.84089662970566\n",
      "random num: 15372\n",
      "> is the best tea i have ever had !   you do not need sweetner of any kind and it tastes wonderful , i highly recommend this product !\n",
      "= \n",
      "tensor([[   2,   13,   15,    5,   46,   45,    6,   26,  112,   72,   11,   12,\n",
      "           28,   52,   22,  229, 1522,   17,  137,  353,    7,   10,   99,  138,\n",
      "            3]])\n",
      "tensor([[   2, 6912,  362,    3,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1]])\n",
      "tensor([25])\n",
      "tensor([4])\n",
      "  <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "Predictions of shape torch.Size([1, 24, 8464]) tensor([[[1.2947e-05, 4.0425e-08, 2.6654e-08,  ..., 1.2413e-08,\n",
      "          3.1879e-08, 1.3819e-08],\n",
      "         [1.2886e-05, 4.0418e-08, 2.6563e-08,  ..., 1.2346e-08,\n",
      "          3.1715e-08, 1.3850e-08],\n",
      "         [1.2893e-05, 4.0463e-08, 2.6585e-08,  ..., 1.2355e-08,\n",
      "          3.1735e-08, 1.3867e-08],\n",
      "         ...,\n",
      "         [1.3467e-05, 4.3288e-08, 2.8270e-08,  ..., 1.3193e-08,\n",
      "          3.3704e-08, 1.4749e-08],\n",
      "         [1.3532e-05, 4.3633e-08, 2.8471e-08,  ..., 1.3298e-08,\n",
      "          3.3942e-08, 1.4856e-08],\n",
      "         [1.3547e-05, 4.3697e-08, 2.8511e-08,  ..., 1.3316e-08,\n",
      "          3.3987e-08, 1.4876e-08]]])\n",
      "Predicted seq of shape torch.Size([1, 24]) tensor([[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]])\n",
      "Target of shape torch.Size([1, 24]) tensor([[6912,  362,    3,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1]])\n",
      "random num: 14290\n",
      "> coffee ; we love green mountain .   my fiance loves blueberry coffee , but it 's hard to get around the holidays and his birthday , so i was happy to find it !   ( and it makes our kitchen smell awesome when it 's brewed !\n",
      "= \n",
      "tensor([[   2,   19,   33,  313,   55,   31,  208,  571,    4,   12,   18, 5607,\n",
      "           83,  966,   33,    8,   23,   10,   36,  175,   14,   71,  372,    5,\n",
      "            3]])\n",
      "tensor([[ 2, 19, 11,  3,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  1]])\n",
      "tensor([25])\n",
      "tensor([4])\n",
      "  <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "Predictions of shape torch.Size([1, 24, 8464]) tensor([[[1.2893e-05, 4.0213e-08, 2.6513e-08,  ..., 1.2332e-08,\n",
      "          3.1716e-08, 1.3726e-08],\n",
      "         [1.2818e-05, 4.0215e-08, 2.6399e-08,  ..., 1.2261e-08,\n",
      "          3.1510e-08, 1.3746e-08],\n",
      "         [1.2855e-05, 4.0500e-08, 2.6548e-08,  ..., 1.2334e-08,\n",
      "          3.1682e-08, 1.3828e-08],\n",
      "         ...,\n",
      "         [1.3552e-05, 4.3969e-08, 2.8600e-08,  ..., 1.3370e-08,\n",
      "          3.4087e-08, 1.4911e-08],\n",
      "         [1.3619e-05, 4.4324e-08, 2.8806e-08,  ..., 1.3477e-08,\n",
      "          3.4330e-08, 1.5020e-08],\n",
      "         [1.3687e-05, 4.4680e-08, 2.9013e-08,  ..., 1.3585e-08,\n",
      "          3.4574e-08, 1.5130e-08]]])\n",
      "Predicted seq of shape torch.Size([1, 24]) tensor([[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]])\n",
      "Target of shape torch.Size([1, 24]) tensor([[19, 11,  3,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1]])\n",
      "random num: 8605\n",
      "> is a great product , especially if you are not eating gluten.<br />my favorite is the onion .   we do n't eat any other type\n",
      "= 's gone\n",
      "tensor([[   2,   13,   15,    9,   19,   40,    8,  375,   68,   28,   24,   22,\n",
      "          292,    0, 1247,   87,   15,    5, 1343,    4,   12,   55,   52,   37,\n",
      "            3]])\n",
      "tensor([[   2, 2674,   36,  704,  481,    3,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1]])\n",
      "tensor([25])\n",
      "tensor([6])\n",
      "  <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "Predictions of shape torch.Size([1, 24, 8464]) tensor([[[1.2924e-05, 4.0301e-08, 2.6589e-08,  ..., 1.2371e-08,\n",
      "          3.1817e-08, 1.3751e-08],\n",
      "         [1.2840e-05, 4.0334e-08, 2.6481e-08,  ..., 1.2300e-08,\n",
      "          3.1639e-08, 1.3785e-08],\n",
      "         [1.2858e-05, 4.0480e-08, 2.6553e-08,  ..., 1.2332e-08,\n",
      "          3.1712e-08, 1.3830e-08],\n",
      "         ...,\n",
      "         [1.3546e-05, 4.3890e-08, 2.8570e-08,  ..., 1.3348e-08,\n",
      "          3.4077e-08, 1.4898e-08],\n",
      "         [1.3612e-05, 4.4239e-08, 2.8773e-08,  ..., 1.3454e-08,\n",
      "          3.4316e-08, 1.5006e-08],\n",
      "         [1.3628e-05, 4.4305e-08, 2.8815e-08,  ..., 1.3472e-08,\n",
      "          3.4364e-08, 1.5027e-08]]])\n",
      "Predicted seq of shape torch.Size([1, 24]) tensor([[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]])\n",
      "Target of shape torch.Size([1, 24]) tensor([[2674,   36,  704,  481,    3,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1]])\n",
      "random num: 14369\n",
      "> this is exactly what i had wanted however i had paid for 3 to 5 shipping and to get the item rushed , the company did not do as instructed,,,,,very disappointed .   i would not order from them\n",
      "= ttooooooooooo   long to recieve this\n",
      "tensor([[  2, 600,  13,  15, 488,  94,   6,  72, 467, 304,   6,  72, 872,  16,\n",
      "         268,  14, 290, 205,   7,  14,  71,   5, 235,   0,   3]])\n",
      "tensor([[   2,  654,    0,   12,  255,   14, 3431,   13,  106,    3,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1]])\n",
      "tensor([25])\n",
      "tensor([10])\n",
      "  <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "Predictions of shape torch.Size([1, 24, 8464]) tensor([[[1.2911e-05, 4.0255e-08, 2.6547e-08,  ..., 1.2352e-08,\n",
      "          3.1761e-08, 1.3739e-08],\n",
      "         [1.2812e-05, 4.0175e-08, 2.6378e-08,  ..., 1.2254e-08,\n",
      "          3.1509e-08, 1.3740e-08],\n",
      "         [1.2853e-05, 4.0419e-08, 2.6525e-08,  ..., 1.2321e-08,\n",
      "          3.1680e-08, 1.3814e-08],\n",
      "         ...,\n",
      "         [1.3598e-05, 4.4198e-08, 2.8749e-08,  ..., 1.3448e-08,\n",
      "          3.4279e-08, 1.4986e-08],\n",
      "         [1.3613e-05, 4.4265e-08, 2.8790e-08,  ..., 1.3467e-08,\n",
      "          3.4326e-08, 1.5008e-08],\n",
      "         [1.3628e-05, 4.4332e-08, 2.8832e-08,  ..., 1.3486e-08,\n",
      "          3.4374e-08, 1.5029e-08]]])\n",
      "Predicted seq of shape torch.Size([1, 24]) tensor([[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]])\n",
      "Target of shape torch.Size([1, 24]) tensor([[ 654,    0,   12,  255,   14, 3431,   13,  106,    3,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1]])\n",
      "random num: 10877\n",
      "> 've been using antica italia for many years .   other balsamics seem weak in comparison .   this is a strong , full - bodied balsamic vinegar\n",
      "= balsamic\n",
      "tensor([[   2,    6,  103,  118,  283,    0,    0,   16,  189,  224,    4,   12,\n",
      "           90,    0,  490,  519,   20, 1611,    4,   12,   13,   15,    9,  160,\n",
      "            3]])\n",
      "tensor([[   2,   19, 1941,   11,    3,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1]])\n",
      "tensor([25])\n",
      "tensor([5])\n",
      "  <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "Predictions of shape torch.Size([1, 24, 8464]) tensor([[[1.2912e-05, 4.0285e-08, 2.6573e-08,  ..., 1.2354e-08,\n",
      "          3.1798e-08, 1.3758e-08],\n",
      "         [1.2835e-05, 4.0282e-08, 2.6454e-08,  ..., 1.2281e-08,\n",
      "          3.1583e-08, 1.3777e-08],\n",
      "         [1.2894e-05, 4.0592e-08, 2.6629e-08,  ..., 1.2367e-08,\n",
      "          3.1780e-08, 1.3871e-08],\n",
      "         ...,\n",
      "         [1.3571e-05, 4.4051e-08, 2.8660e-08,  ..., 1.3385e-08,\n",
      "          3.4161e-08, 1.4947e-08],\n",
      "         [1.3587e-05, 4.4120e-08, 2.8703e-08,  ..., 1.3404e-08,\n",
      "          3.4210e-08, 1.4969e-08],\n",
      "         [1.3654e-05, 4.4478e-08, 2.8911e-08,  ..., 1.3512e-08,\n",
      "          3.4455e-08, 1.5080e-08]]])\n",
      "Predicted seq of shape torch.Size([1, 24]) tensor([[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]])\n",
      "Target of shape torch.Size([1, 24]) tensor([[  19, 1941,   11,    3,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1]])\n",
      "random num: 15918\n",
      "> product works better than other forms of magnesium i 've tried .   i was a little over - zealous with it when i first got it and got the runs .   now i use a serving size before bed , i sleep well , i feel relaxed and things move well\n",
      "= it , more absorbable\n",
      "tensor([[   2,   13,   40,  333,   82,   66,   90, 6397,   17, 2768,    6,  103,\n",
      "          102,    4,   12,    6,   29,    9,   89,  182,   38,    0,   30,   10,\n",
      "            3]])\n",
      "tensor([[   2,   32,   10,    8,   67, 5389,   70,    3,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1]])\n",
      "tensor([25])\n",
      "tensor([8])\n",
      "  <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "Predictions of shape torch.Size([1, 24, 8464]) tensor([[[1.2936e-05, 4.0315e-08, 2.6606e-08,  ..., 1.2384e-08,\n",
      "          3.1837e-08, 1.3758e-08],\n",
      "         [1.2828e-05, 4.0209e-08, 2.6414e-08,  ..., 1.2272e-08,\n",
      "          3.1544e-08, 1.3754e-08],\n",
      "         [1.2856e-05, 4.0404e-08, 2.6522e-08,  ..., 1.2321e-08,\n",
      "          3.1657e-08, 1.3815e-08],\n",
      "         ...,\n",
      "         [1.3599e-05, 4.4082e-08, 2.8690e-08,  ..., 1.3405e-08,\n",
      "          3.4143e-08, 1.4945e-08],\n",
      "         [1.3614e-05, 4.4147e-08, 2.8730e-08,  ..., 1.3424e-08,\n",
      "          3.4189e-08, 1.4966e-08],\n",
      "         [1.3629e-05, 4.4212e-08, 2.8771e-08,  ..., 1.3442e-08,\n",
      "          3.4236e-08, 1.4987e-08]]])\n",
      "Predicted seq of shape torch.Size([1, 24]) tensor([[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]])\n",
      "Target of shape torch.Size([1, 24]) tensor([[  32,   10,    8,   67, 5389,   70,    3,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1]])\n",
      "random num: 15447\n",
      "> vanilla beans were fresh when i received them with an outstanding flavor .   i made homemade vanilla ice cream and using store bought would have never worked .\n",
      "= vanilla\n",
      "tensor([[  2,   5, 287, 405,  81, 162,  74,   6, 248,  39,  30, 109, 988,  47,\n",
      "           4,  12,   6, 167, 800, 287, 468, 425,   7, 283,   3]])\n",
      "tensor([[   2, 4035,  287,  405,    3,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1]])\n",
      "tensor([25])\n",
      "tensor([5])\n",
      "  <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "Predictions of shape torch.Size([1, 24, 8464]) tensor([[[1.2931e-05, 4.0359e-08, 2.6615e-08,  ..., 1.2385e-08,\n",
      "          3.1837e-08, 1.3781e-08],\n",
      "         [1.2851e-05, 4.0325e-08, 2.6475e-08,  ..., 1.2301e-08,\n",
      "          3.1609e-08, 1.3800e-08],\n",
      "         [1.2862e-05, 4.0380e-08, 2.6507e-08,  ..., 1.2314e-08,\n",
      "          3.1644e-08, 1.3819e-08],\n",
      "         ...,\n",
      "         [1.3660e-05, 4.4439e-08, 2.8898e-08,  ..., 1.3529e-08,\n",
      "          3.4458e-08, 1.5081e-08],\n",
      "         [1.3727e-05, 4.4798e-08, 2.9107e-08,  ..., 1.3638e-08,\n",
      "          3.4704e-08, 1.5192e-08],\n",
      "         [1.3743e-05, 4.4866e-08, 2.9149e-08,  ..., 1.3657e-08,\n",
      "          3.4752e-08, 1.5213e-08]]])\n",
      "Predicted seq of shape torch.Size([1, 24]) tensor([[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]])\n",
      "Target of shape torch.Size([1, 24]) tensor([[4035,  287,  405,    3,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1]])\n",
      "random num: 9737\n",
      "> wife 's favorite tea ever . she was very dissapointed when local stores stopped selling it . glad to find it on amazon . this purchase earned me a lot of atta - boys\n",
      "= green\n",
      "tensor([[   2,   18,  465,   36,   87,   45,  112,    4,  132,   29,   34, 1768,\n",
      "           74,  265,  213,  817, 1205,   10,    4,  348,   14,   78,   10,   42,\n",
      "            3]])\n",
      "tensor([[   2, 7834,  208,   45,    3,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1]])\n",
      "tensor([25])\n",
      "tensor([5])\n",
      "  <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "Predictions of shape torch.Size([1, 24, 8464]) tensor([[[1.2935e-05, 4.0396e-08, 2.6651e-08,  ..., 1.2388e-08,\n",
      "          3.1884e-08, 1.3790e-08],\n",
      "         [1.2819e-05, 4.0205e-08, 2.6417e-08,  ..., 1.2261e-08,\n",
      "          3.1551e-08, 1.3759e-08],\n",
      "         [1.2829e-05, 4.0260e-08, 2.6447e-08,  ..., 1.2273e-08,\n",
      "          3.1584e-08, 1.3778e-08],\n",
      "         ...,\n",
      "         [1.3400e-05, 4.3021e-08, 2.8095e-08,  ..., 1.3092e-08,\n",
      "          3.3521e-08, 1.4650e-08],\n",
      "         [1.3471e-05, 4.3398e-08, 2.8314e-08,  ..., 1.3206e-08,\n",
      "          3.3780e-08, 1.4767e-08],\n",
      "         [1.3543e-05, 4.3775e-08, 2.8534e-08,  ..., 1.3320e-08,\n",
      "          3.4041e-08, 1.4884e-08]]])\n",
      "Predicted seq of shape torch.Size([1, 24]) tensor([[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]])\n",
      "Target of shape torch.Size([1, 24]) tensor([[7834,  208,   45,    3,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1]])\n",
      "random num: 17196\n",
      "> like this for our decaf coffee . it is smooth , good flavor , no bitterness , no acidic taste\n",
      "= 's house blend k - cup decaf\n",
      "tensor([[   2,   55,   32,   13,   16,  113,  382,   33,    4,   10,   15,  260,\n",
      "            8,   21,   47,    8,   73,  993,    8,   73, 1422,   43,    4,    3,\n",
      "            1]])\n",
      "tensor([[   2, 1220,   36,  362,  274,  140,   38,   95,  382,   33,    3,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1]])\n",
      "tensor([24])\n",
      "tensor([11])\n",
      "  <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "Predictions of shape torch.Size([1, 24, 8464]) tensor([[[1.2924e-05, 4.0298e-08, 2.6595e-08,  ..., 1.2370e-08,\n",
      "          3.1819e-08, 1.3750e-08],\n",
      "         [1.2791e-05, 4.0076e-08, 2.6327e-08,  ..., 1.2224e-08,\n",
      "          3.1439e-08, 1.3713e-08],\n",
      "         [1.2801e-05, 4.0129e-08, 2.6355e-08,  ..., 1.2235e-08,\n",
      "          3.1468e-08, 1.3732e-08],\n",
      "         ...,\n",
      "         [1.3627e-05, 4.4347e-08, 2.8850e-08,  ..., 1.3493e-08,\n",
      "          3.4398e-08, 1.5038e-08],\n",
      "         [1.3696e-05, 4.4716e-08, 2.9065e-08,  ..., 1.3605e-08,\n",
      "          3.4651e-08, 1.5151e-08],\n",
      "         [1.3765e-05, 4.5087e-08, 2.9280e-08,  ..., 1.3717e-08,\n",
      "          3.4906e-08, 1.5265e-08]]])\n",
      "Predicted seq of shape torch.Size([1, 24]) tensor([[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]])\n",
      "Target of shape torch.Size([1, 24]) tensor([[1220,   36,  362,  274,  140,   38,   95,  382,   33,    3,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1]])\n",
      "random num: 21679\n",
      "> bowls slow down the dog 's eating habit .   they are easy to clean and stack .   i also like the way the rubber on the bottom keeps the bowls from skidding\n",
      "= \n",
      "tensor([[   2,   25, 3042, 1719,  368,    5,   79,   36,  292, 3371,    4,   12,\n",
      "           27,   24,  144,   14,  575,    7, 5276,    4,   12,    6,  122,   32,\n",
      "            3]])\n",
      "tensor([[   2, 1719, 4267,    3,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1]])\n",
      "tensor([25])\n",
      "tensor([4])\n",
      "  <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "Predictions of shape torch.Size([1, 24, 8464]) tensor([[[1.2929e-05, 4.0295e-08, 2.6591e-08,  ..., 1.2376e-08,\n",
      "          3.1817e-08, 1.3748e-08],\n",
      "         [1.2793e-05, 4.0065e-08, 2.6316e-08,  ..., 1.2226e-08,\n",
      "          3.1428e-08, 1.3710e-08],\n",
      "         [1.2814e-05, 4.0211e-08, 2.6396e-08,  ..., 1.2264e-08,\n",
      "          3.1511e-08, 1.3754e-08],\n",
      "         ...,\n",
      "         [1.3662e-05, 4.4532e-08, 2.8934e-08,  ..., 1.3561e-08,\n",
      "          3.4484e-08, 1.5096e-08],\n",
      "         [1.3725e-05, 4.4867e-08, 2.9129e-08,  ..., 1.3663e-08,\n",
      "          3.4713e-08, 1.5199e-08],\n",
      "         [1.3789e-05, 4.5205e-08, 2.9324e-08,  ..., 1.3765e-08,\n",
      "          3.4943e-08, 1.5303e-08]]])\n",
      "Predicted seq of shape torch.Size([1, 24]) tensor([[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]])\n",
      "Target of shape torch.Size([1, 24]) tensor([[1719, 4267,    3,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=1)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "def loss_fn(predicted, target):\n",
    "    \n",
    "    \n",
    "\n",
    "for epoch in tqdm(range(1, (config.get(\"epochs\") + 1))):\n",
    "\n",
    "  train_loss = train(model, train_iter, criterion, optimizer)\n",
    "  dev_loss = eval(model, val_iter, criterion)\n",
    "  print(f\"\\nTraining loss for epoch {epoch}:\\t {train_loss}\")\n",
    "  print(f\"\\nDev loss for epoch {epoch}:\\t {dev_loss}\")\n",
    "\n",
    "  random_eval(model, train_data, 10)\n",
    "  \n",
    "# path = \"/content/gdrive/My Drive/data/amazon/models/model.pt\"\n",
    "# torch.save(model, path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "noXs_OtXsLLy"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8464"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(TEXT.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = torch.randint(10, (4,10))\n",
    "target  = torch.randint(10, (4,10)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[9, 5, 1, 6, 6, 8, 3, 1, 9, 4],\n",
       "        [8, 7, 5, 6, 3, 1, 3, 0, 0, 9],\n",
       "        [2, 5, 3, 6, 6, 1, 5, 2, 4, 9],\n",
       "        [2, 7, 7, 4, 6, 7, 5, 0, 6, 0]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 6, 1, 7, 4, 6, 9, 2, 8, 0],\n",
       "        [8, 3, 9, 5, 9, 8, 8, 5, 6, 4],\n",
       "        [1, 2, 4, 3, 3, 8, 2, 6, 6, 4],\n",
       "        [1, 1, 1, 2, 8, 4, 0, 6, 6, 4]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_lens = [4, 6, 3, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 7, 6, 6, 1, 1, 1, 1, 1, 1],\n",
       "        [2, 5, 2, 3, 1, 1, 1, 1, 1, 1],\n",
       "        [2, 3, 4, 4, 1, 1, 1, 1, 1, 1],\n",
       "        [5, 3, 9, 7, 1, 1, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " target[:,4:] = 1\n",
    " target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create mask\n",
    "bs, max_len = target.size() \n",
    "mask = torch.zeros(bs, max_len)\n",
    "for i in range(bs):\n",
    "    mask[i, :target_lens[i]] = 1\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 6, 1, 7, 4, 6, 9, 2, 8, 0],\n",
       "        [8, 3, 9, 5, 9, 8, 8, 5, 6, 4],\n",
       "        [1, 2, 4, 3, 3, 8, 2, 6, 6, 4],\n",
       "        [1, 1, 1, 2, 8, 4, 0, 6, 6, 4]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 6., 1., 7., 0., 0., 0., 0., 0., 0.],\n",
       "        [8., 3., 9., 5., 9., 8., 0., 0., 0., 0.],\n",
       "        [1., 2., 4., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 2., 8., 4., 0., 6., 6., 4.]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target * mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Truncate first SOS prediction and reshape [B, T, H] > [B*T, H]\n",
    "    predictions = predictions[:, 1:, :].view(-1, predictions.size(2))\n",
    "    target = target[:, 1:].view(-1)\n",
    "    mask = mask[:, 1:].view(-1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "torch.rand((3,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4258, 0.0387, 0.0075, 0.0979, 0.4808, 0.8143],\n",
       "        [0.5739, 0.2371, 0.4398, 0.4092, 0.3307, 0.1077],\n",
       "        [0.7814, 0.6574, 0.1097, 0.6844, 0.6188, 0.1062],\n",
       "        [0.9651, 0.8787, 0.7334, 0.5664, 0.9096, 0.1484]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=torch.rand(4,6\n",
    "            )\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 3, 0, 5])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind = torch.randint(x.size(-1), (4,))\n",
    "ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0387],\n",
       "        [0.4092],\n",
       "        [0.7814],\n",
       "        [0.1484]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.gather(x, 1, ind.unsqueeze(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "text_summarization2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (seq2seq)",
   "language": "python",
   "name": "seq2seq"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
